{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible learning rate schedule design via callbacks\n",
    "## In this notebook, we explore learning rate schedules, and we use callbacks to develop a method to combine a list of schedules into a single piecewise schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare to implement a learning rate schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all code from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# NOTE: there are some differences in the *_jcat.ipynb vs. the *.ipynb notebooks\n",
    "#      to get this notebook to run, generate the required *.py files from the *_jcat.ipynb versions\n",
    "#      of the preceding notebooks\n",
    "from exp.nb_04 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the MNIST training and validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "n_hidden,batch_size = 50,512\n",
    "n_out = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the training and validation datasets into a DataBunch object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBunch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, batch_size), c=n_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a function that returns a Learner object as a container for model, optimizer, loss_func, and data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_learner(model_func, loss_func, data):\n",
    "    return Learner(*model_func(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Learner and Runner objects, and run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'n_epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-76eb5548e502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAvgStatsCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# run the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'n_epochs'"
     ]
    }
   ],
   "source": [
    "# instantiate a learner object\n",
    "#     recall that get_model builds a model, and returns the model, the optimizer, and the learning rate\n",
    "learn = create_learner(get_model, loss_func, data)\n",
    "# instantiate a runner object\n",
    "run = Runner([AvgStatsCallback([accuracy])])\n",
    "# run the training loop\n",
    "run.fit(learn, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Learner and Runner objects, and run the training loop again, this time changing the learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a learner object\n",
    "learn = create_learner(partial(get_model, learning_rate=0.3), loss_func, data)\n",
    "# instantiate a runner object\n",
    "run = Runner([AvgStatsCallback([accuracy])])\n",
    "# run the training loop for 3 epochs\n",
    "run.fit(learn, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a partial get_model function that takes only a learning rate as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_model_func(learning_rate=0.5): \n",
    "    return partial(get_model, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7202)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning rate schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear learning rate schedule\n",
    "Let's start with a simple linear schedule going from start to end. It returns a function that takes a `pos` argument (going from 0 to 1) such that this function goes from `start` (at `pos=0`) to `end` (at `pos=1`) in a linear fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear schedule\n",
    "def sched_lin(start, end):\n",
    "    def _inner(start, end, pos): \n",
    "        return start + pos*(end-start)\n",
    "    return partial(_inner, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can refactor this with a decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7526)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def annealer(f):\n",
    "    def _inner(start, end): \n",
    "        return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): \n",
    "    return start + pos*(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift-tab works too, in Jupyter!\n",
    "# sched_lin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sched_lin(1,2)\n",
    "f(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other learning rate schedules: cosine, constant, exponential, beta\n",
    "And here are other scheduler functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@annealer\n",
    "# cosine schedule\n",
    "def sched_cos(start, end, pos): \n",
    "    return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "# constant schedule\n",
    "@annealer\n",
    "def sched_no(start, end, pos):  \n",
    "    return start\n",
    "# exponential schedule\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): \n",
    "    return start * (end/start) ** pos\n",
    "\n",
    "# betafunction schedule\n",
    "# note that it's 'cheating' to import the beta function and numpy\n",
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "@annealer\n",
    "def sched_beta(start, end, pos):\n",
    "    # hardwire beta parameters for now\n",
    "    a, b = 2.5, 5.\n",
    "    x = np.linspace(0.,1.0,100)\n",
    "    beta_pdf = beta.pdf(x, a, b)\n",
    "    scaled_pos = pos/(end-start)\n",
    "    scale = 0.8\n",
    "    return scale*np.interp(pos, x, beta_pdf)\n",
    "\n",
    "@annealer\n",
    "#def cos_1cycle_anneal(start, high, end): return [sched_cos(start, high), sched_cos(high, end)]\n",
    "# have to use combine_scheds to make this work; currently it outputs a list of partial functions\n",
    "def sched_cos_1cycle(start, end, pos):\n",
    "    return [sched_cos(start, y_max), sched_cos(y_max, end)]\n",
    "\n",
    "\n",
    "#Add an ndim property to the Tensor class so that tensors can be plotted\n",
    "torch.Tensor.ndim = property(lambda x: len(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=7730)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize these learning rate schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup range of iteration indices\n",
    "n_points = 100\n",
    "start_point = 0\n",
    "indices = torch.arange(0, n_points)\n",
    "\n",
    "# uniformly sample 100 points on the interval [0,1] at step size of  of x_step\n",
    "x_start = 0.01\n",
    "x_end = 1\n",
    "x_step = 0.01\n",
    "points = torch.linspace(x_start,x_end,n_points)\n",
    "\n",
    "# plot the annealing schedules\n",
    "begin_learning_rate = 2.\n",
    "end_learning_rate = 0.01\n",
    "# annealing_labels = \"NO LINEAR COS EXP BETA COS_1CYCLE\".split()\n",
    "# annealing_funcs = [sched_no, sched_lin, sched_cos, sched_exp, sched_beta,sched_cos_1cycle]\n",
    "annealing_labels = \"NO LINEAR COS EXP COS_1CYCLE\".split()\n",
    "annealing_funcs = [sched_no, sched_lin, sched_cos, sched_exp, sched_cos_1cycle]\n",
    "# annealing_labels = \"NO LINEAR COS EXP\".split()\n",
    "# annealing_funcs = [sched_no, sched_lin, sched_cos, sched_exp]\n",
    "for func, label in zip(annealing_funcs, annealing_labels):\n",
    "    annealing_schedule = func(begin_learning_rate, end_learning_rate)\n",
    "    plt.plot(indices, [annealing_schedule(pos) for pos in points], label=label)\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('learning rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Piecewise Schedules by Combining Schedulers \n",
    "In practice, we'll often want to combine different schedulers, the following function creates a piecewise schedule that uses `scheds[i]` for `fracs[i]` of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def combine_scheds(fracs, scheds):\n",
    "    \n",
    "    # check that all fracs are positive and that they sum to 1\n",
    "    assert sum(fracs) == 1.\n",
    "    fracs = tensor([0] + listify(fracs))\n",
    "    assert torch.all(fracs >= 0)\n",
    "    \n",
    "    # relative position (between 0 and 1) at the boundary of each piecewise section of the schedule\n",
    "    cum_fracs = torch.cumsum(fracs, 0)\n",
    "    \n",
    "    def _inner(pos):\n",
    "        # given a relative position pos (between 0 and 1) along the schedule interval\n",
    "        #    if pos doesn't correspond to a section boundary point,\n",
    "        #        compute the index of the nearest section boundary point to its left in cum_fracs;\n",
    "        #    otherwise return the index corresponding to pos in cum_fracs\n",
    "        idx = (pos >= cum_fracs).nonzero().max()\n",
    "        # print('index = ',idx)\n",
    "        # compute the relative position of pos within its piecewise section interval; ranges from 0 to 1\n",
    "        actual_pos = (pos-cum_fracs[idx]) / (cum_fracs[idx+1]-cum_fracs[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    # return the scheduler function\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 1: use 30% of the budget to go from 0.3 to 0.6 following a cosine, then the last 70% of the budget to go from 0.6 to 0.2, still following a cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify and create a piecewise schedule\n",
    "sched = combine_scheds([0.3,0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the piecewise schedule\n",
    "plt.plot(indices, [sched(pos) for pos in points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: use 60% of the budget to go from 0.2 to 0.9 following an exp, the 20% of the budget with a constant, then the last 20% of the budget to go from 0.9 to 0.05, still following an exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify and create a piecewise schedule\n",
    "sched = combine_scheds([0.6,0.2,.2], [sched_exp(0.2, 0.9),sched_no(0.9, 0.9), sched_exp(0.9, 0.05)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the piecewise schedule\n",
    "plt.plot(indices, [sched(pos) for pos in points])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The beta schedule\n",
    "The **beta probability distribution (pdf)** is defined on the interval [0,1], and has a varying shape that is controlled by its two input parameters a and b. \n",
    "\n",
    "When $a$ and $b$ are both greater than 1, the beta function is convex and vanishes at 0 and 1, allowing the construction of an infinite variety of ***smoothly continuous learning rate schedules that start and end with learning rate near zero***. \n",
    "\n",
    "See the purple curve in the first figure above. You can play with the beta schedule by varying the values of $a$ and $b$.\n",
    "\n",
    "ref: https://en.wikipedia.org/wiki/Beta_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sched = sched_beta(0.1,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flexible learning rate schedule design via callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorder and ParamScheduler callbacks\n",
    "We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Recorder(Callback):\n",
    "    def begin_fit(self): \n",
    "        self.lrs,self.losses = [],[]\n",
    "\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.lrs.append(self.opt.param_groups[-1]['learning_rate'])\n",
    "        self.losses.append(self.loss.detach().cpu())        \n",
    "\n",
    "    def plot_lr  (self): \n",
    "        plt.plot(self.lrs)\n",
    "    def plot_loss(self): \n",
    "        plt.plot(self.losses)\n",
    "\n",
    "class ParamScheduler(Callback):\n",
    "    # order parameter\n",
    "    _order=1\n",
    "    def __init__(self, pname, sched_func): \n",
    "        self.pname,self.sched_func = pname,sched_func\n",
    "\n",
    "    def set_param(self):\n",
    "        for param_group in self.opt.param_groups:\n",
    "            # print(self.epoch, self.n_epochs_float)\n",
    "            # param_group[self.pname] = self.sched_func((self.epoch+self.n_epochs_float)/(self.epoch+1))\n",
    "            param_group[self.pname] = self.sched_func((self.n_epochs_float+self.epoch)/self.n_epochs)\n",
    "            \n",
    "    def begin_batch(self): \n",
    "        if self.in_train: \n",
    "            self.set_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it for training quite easily..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of callbacks we want to use\n",
    "callback_funcs = [Recorder,\n",
    "        partial(AvgStatsCallback,accuracy),\n",
    "        partial(ParamScheduler, 'learning_rate', sched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Learner and Runner objects\n",
    "learn = create_learner(get_model_func(learning_rate = 0.3), loss_func, data)\n",
    "run = Runner(callback_funcs=callback_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the schedule\n",
    "run.fit(learn, n_epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then check with our recorder if the learning rate followed the right schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning rate schedule\n",
    "run.recorder.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss\n",
    "run.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py 05_anneal_jcat.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
