{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib color map\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns of training data\n",
    "n,m = x_train.shape\n",
    "# number of possible output labels\n",
    "c = y_train.max()+1\n",
    "# size of the hidden layer\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fully connected network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out)]\n",
    "    # sequentially apply layers and return the output activations\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = Model(n_in = m, n_hidden = nh, n_out = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.143938, -0.050673,  0.157736, -0.081497,  0.154572, -0.182136, -0.06047 ,  0.066624, -0.140994, -0.198018],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict training labels\n",
    "pred = model(x_train)\n",
    "\n",
    "# look at the output activations for the first training example\n",
    "import numpy as np\n",
    "pred.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss is the usual loss function for multi-label classification problems. Since cross-entropy loss is computed from the probabilities of the predicted classes, we must first convert the output activations to probabilities; this is accomplished by applying the softmax function to the output activations.\n",
    "\n",
    "Softmax is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "In the above formulas, $i$ indexes the output activation node.\n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log(softmax)\n",
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape  torch.Size([50000, 10])\n",
      "\n",
      "output log probabilities:  [-2.218516 -2.299908 -2.132604 -2.449856 -2.189671 -2.389762 -2.181355 -2.300313 -2.420501 -2.519137]\n",
      "\n",
      "output probabilities:  [0.10877  0.100268 0.118528 0.086306 0.111954 0.091651 0.112888 0.100228 0.088877 0.080529]\n",
      "\n",
      "sum of probabilities is  0.9999999\n",
      "\n",
      "predicted digit is  2\n"
     ]
    }
   ],
   "source": [
    "# applying softmax to the output activations converts them to log probabilities\n",
    "sm_pred = log_softmax(pred)\n",
    "print('\\nshape ',sm_pred.shape)\n",
    "\n",
    "# look at the first example in the training set\n",
    "print('\\noutput log probabilities: ',sm_pred.detach().numpy()[1,:])\n",
    "print('\\noutput probabilities: ',np.exp(sm_pred.detach().numpy()[1,:]))\n",
    "\n",
    "# check that sum of probabilities is one\n",
    "print('\\nsum of probabilities is ',np.exp(sm_pred.detach().numpy()[1,:]).sum())\n",
    "\n",
    "# the predicted digit is the output with the highest probability\n",
    "print('\\npredicted digit is ',np.argmax(np.exp(sm_pred.detach().numpy()[1,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss\n",
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ \\ell = -\\sum x\\, \\log p(x)$$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as \n",
    "\n",
    "$$ \\ell = -\\log(p_{i})$$\n",
    "where $i$ is the index of the true class label.\n",
    "\n",
    "And the loss function is the mean cross-entropy loss over all the examples:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{\\sum \\ell}{N}$$\n",
    "where $N$ is the number of examples in the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the cross-entropy loss function using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link. \n",
    "\n",
    "The next few cells provide examples of indexing, then we'll go on to implement the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train (target) shape is  torch.Size([50000])\n",
      "\n",
      "sm_pred (input) shape is  torch.Size([50000, 10])\n"
     ]
    }
   ],
   "source": [
    "# shapes of training set predictions output and of the training labels\n",
    "print('\\ny_train (target) shape is ',y_train.shape)\n",
    "print('\\nsm_pred (input) shape is ',sm_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing, example 1\n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4448, -2.2185, -2.1556], grad_fn=<IndexBackward>)\n",
      "tensor(-2.4448, grad_fn=<SelectBackward>)\n",
      "tensor(-2.2185, grad_fn=<SelectBackward>)\n",
      "tensor(-2.1556, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# indexing, example 2\n",
    "print(sm_pred[[0,1,2], [5,0,4]]) \n",
    "print(sm_pred[0][5])\n",
    "print(sm_pred[1][0])\n",
    "print(sm_pred[2][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4066, -2.3134, -2.1049, -2.3442, -2.1081, -2.4448],\n",
      "        [-2.2185, -2.2999, -2.1326, -2.4499, -2.1897, -2.3898],\n",
      "        [-2.2147, -2.3142, -2.1566, -2.4115, -2.1556, -2.3468]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([-2.4066, -2.3134, -2.1049, -2.3442, -2.1081, -2.4448],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([-2.2185, -2.2999, -2.1326, -2.4499, -2.1897, -2.3898],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([-2.2147, -2.3142, -2.1566, -2.4115, -2.1556, -2.3468],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# indexing, example 3\n",
    "print(sm_pred[[0,1,2], :6])\n",
    "print(sm_pred[0,:6])\n",
    "print(sm_pred[1,:6])\n",
    "print(sm_pred[2,:6])\n",
    "# print(sm_pred[[0,1,2], 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nll (negative log likelihood) loss function\n",
    "#    nll is our implementation of the cross-entropy loss function\n",
    "#    -- computes a vector of the predicted log probabilities corresponding to the true labels for each example\n",
    "#    -- outputs the negative mean of this vector, which is the cross-entropy loss function\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3170, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the nll (or cross-entropy) loss function\n",
    "loss = nll(sm_pred, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor log_softmax\n",
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that nll using the refactored log_softmax is the same as before\n",
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a problem with the logsumexp function: overflow can occur if the exponents become large positive numbers. \n",
    "The [LogSumExp trick] provides a brilliant way to compute the log of the sum of exponentials in a more stable way. (https://en.wikipedia.org/wiki/LogSumExp). The idea is to express logsumexp in the following way:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where $a$ is the maximum of the $x_{j}$.\n",
    "\n",
    "Note that because of the clever choice of $a$, all the exponents on the right-hand side are $\\le 0$. Recall that the exponential of a negative number is always a positive number that is less than $1$. So the expression on the right hand side contains no exponentials of large positive numbers. This solves the overflow problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LogSumExp trick for numerical stability\n",
    "def logsumexp(x):\n",
    "    # index (-1) indicates the last dimension, in this case, it's the same as (1), which is across rows\n",
    "    # but pytorch max returns a vector of values and also a vector of the column indices corresponding to those values \n",
    "    # the [0] at the end of the line below is necessary to select the values\n",
    "    m = x.max(-1)[0]\n",
    "    # m is a vector; to convert it to a column vector, i.e. a matrix with one column, use m[:,None] \n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence between our logsumexp function and python's implementation\n",
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use it for our `log_softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor log_softmax again\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of nll and loss\n",
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation of nll_loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of pytorch's implementation of nll and loss\n",
    "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of python's corss-entropy and loss\n",
    "test_near(F.cross_entropy(pred, y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are allowed to use Pytorch implementation of loss function\n",
    "#    inputs preds, labels\n",
    "#    where preds are the activation outputs\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# accuracy is the fraction of correct predictions\n",
    "# predicted class is torch.argmax(prediction_probs, dim=1)==target)\n",
    "def accuracy(prediction_probs, target): \n",
    "    return (torch.argmax(prediction_probs, dim=1)==target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1439, -0.0507,  0.1577, -0.0815,  0.1546, -0.1821, -0.0605,  0.0666,\n",
       "         -0.1410, -0.1980], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a mini-batch and run the model to get predictions\n",
    "bs=64                  # batch size\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions (activation outputs)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3139, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the loss function\n",
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy for a forward pass on a single batch\n",
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "lr = 0.5   # learning rate\n",
    "epochs = 1 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial training loop\n",
    "def fit(model,loss_func,epochs,bs,lr,n,x_train,y_train):\n",
    "    # loop over epochs; an epoch consists of all n samples\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches; a batch is a subset of samples specified by bs, e.g., 64 samples if bs = 64\n",
    "        #   batch index goes from 0 to (n-1)/bs + 1, so the last batch can have fewer than bs samples\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            # set_trace()\n",
    "            # index of first sample in batch number i \n",
    "            start_i = i*bs\n",
    "            # index after last sample in batch number i\n",
    "            end_i = start_i+bs\n",
    "            # get the current batch\n",
    "            xb = x_train[start_i:end_i] # data points\n",
    "            yb = y_train[start_i:end_i] # labels\n",
    "            # make a forward pass, then compute the loss function\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            # compute the gradients of the loss function with respect to the parameters in each layer\n",
    "            loss.backward()\n",
    "            # loop over the layers, applying updates for each parameter\n",
    "            #   temporarily set requires_grad flags (whose default values are True) to False,\n",
    "            #      since we don't want to track the gradients while we are making empirical updates\n",
    "            #   after the updates, reset the gradients to zero to prepare for the next batch\n",
    "            with torch.no_grad(): \n",
    "                for l in model.layers:\n",
    "                    if hasattr(l, 'weight'):\n",
    "                        # update the weights and biases using SGD\n",
    "                        l.weight -= l.weight.grad * lr\n",
    "                        l.bias   -= l.bias.grad   * lr\n",
    "                        # zero the gradients\n",
    "                        l.weight.grad.zero_()\n",
    "                        l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "# a linear layer has weight and bias\n",
    "print(hasattr(nn.Linear(nh,c.numpy()),'weight'),hasattr(nn.Linear(nh,c.numpy()),'bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "# a ReLU layer has no parameters\n",
    "print(hasattr(nn.ReLU(),'weight'),hasattr(nn.ReLU(),'bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1387, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit(model,loss_func,epochs,bs,lr,n,x_train,y_train)\n",
    "# loss function and accuracy after 1 epoch of training with bs = 64 and lr = 0.5\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nn.Module.__setattr__` and move relu to functional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original version of Model(), the starting point for refactoring\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # inherits from nn.Module, and super().__init__() allows access to all the methods and attributes of nn.Module\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out)]\n",
    "    # sequentially apply layers and return the output activations\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor Model() to eliminate the loop over layers\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # inherits from nn.Module, and super().__init__() allows access to the methods and attributes of nn.Module\n",
    "        #    if no arguments were passed, the line below would just be super() \n",
    "        super().__init__()\n",
    "        # implicitly use __setattr__ to set module attributes\n",
    "        self.l1 = nn.Linear(n_in,n_hidden)\n",
    "        self.l2 = nn.Linear(n_hidden,n_out)\n",
    "    # avoid use of loop\n",
    "    #    move ReLU to functional\n",
    "    def __call__(self, x): \n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Model\n",
    "model = Model(n_in = m, n_hidden = nh, n_out = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# model.named_children is a generator over child modules, returns name and module itself\n",
    "# children of self\n",
    "for name,l in model.named_children(): print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns named_children names and modules\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one of the children modules\n",
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor parameter updates using model.parameters()\n",
    "# training loop\n",
    "def fit():\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            # compute loss function for this batch\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD and zero out the gradients\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2259, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# record the final loss and accuracy \n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class duplicates the functionality of Model without Pytorch\n",
    "#    notice it does not inherit from a parent class\n",
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {} #dictionary\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "    \n",
    "    # all attributes of the class that can be shared\n",
    "    #    generally, variable names starting with '_' are private to the class or function\n",
    "    def __setattr__(self,k,v):\n",
    "        # loop over self.modules, registering each as parameters\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        # what is the super class being inherited from?\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    # return the _modules dictionary\n",
    "    #   f-string is evaluated at run-time\n",
    "    def __repr__(self): return f'{self._modules}' \n",
    "    \n",
    "    # generator yields parameters for each module\n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate DummyModule()\n",
    "mdl = DummyModule(m,nh,10)\n",
    "# contents of mdl has the two linear layers\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases\n",
    "#    shapes show that weights are stored as their transposes! \n",
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the original `layers` approach, but we have to register the modules.\n",
    "When a module is 'registered', i.e. added as an attribute of the class, then its weights and biases are incorporated as model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layers to be implemented in the Model\n",
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of original Model, adds modules from an input list\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        # note: the expresssion inside {} in the f-string is replaced with its value at run-time\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Model\n",
    "#     layers is a list of object of class nn.Module()\n",
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model contains all three layers\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor using nn.ModuleList()\n",
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        # self.layers can act as an iterable\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate SequentialModel\n",
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2244, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute a training loop\n",
    "fit()\n",
    "# record the final loss and accuracy \n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch nn.Sequential(), instantiate a model\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1657, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute a training loop, record the final loss and accuracy \n",
    "#     why are loss and accuracy worse than with SequentialModel()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look inside the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an optimizer\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() function\n",
    "# training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            # compute predictions and loss function for this batch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD, and zero out the gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2765, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model and optimizer\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3194, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()\n",
    "# compute loss function for a batch\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1349, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized tests can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# returns a tuple of data and labels\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() once more\n",
    "# training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "            # compute predictions and loss function for this batch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD, and zero out the gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0700, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "for xb,yb in train_dl:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    # bs is batch size, and ds is the data set\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        # iterate through data set sequentially generating the next batch \n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put training and validation data in iterables that generate next batch\n",
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the first batch\n",
    "xb,yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs,28*28)\n",
    "assert yb.shape==(bs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the first image in the first batch\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() using DataLoader class\n",
    "# training loop\n",
    "def fit():\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches generated by DataLoader\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0861, grad_fn=<NllLossBackward>), tensor(0.9844))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator yields indices for random batches from data set\n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # torch.randperm(n) returns a random permutation of integers 0:n-1\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset takes two inputs: *train_ds means pass both elements of train_ds, i.e. the training examples and the labels\n",
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 10 examples and their labels\n",
    "# print(train_ds[:10])\n",
    "# print(len(train_ds[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches with no shuffle, bs = 3\n",
    "# returns indexes of examples\n",
    "# note the last batch has only 1 element\n",
    "s = Sampler(small_ds,3,False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([5, 3, 0]), tensor([1, 9, 7]), tensor([8, 6, 2]), tensor([4])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches, with shuffle\n",
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([7, 4, 8]), tensor([1, 5, 3]), tensor([0, 6, 2]), tensor([9])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches again, with shuffle\n",
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    # returns a batch dataset in the form of tuples of x and y\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "# refactor DataLoader()\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # this is an iterator that yields batches\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "<class 'tuple'>\n",
      "(tensor(5), tensor(0), tensor(4), tensor(1), tensor(9), tensor(2), tensor(1), tensor(3), tensor(1), tensor(4))\n"
     ]
    }
   ],
   "source": [
    "# zip returns the xs and ys of a batch in tuples of tensors\n",
    "xs,ys = zip(*small_ds)\n",
    "print(len(xs),len(ys))\n",
    "print(type(xs))\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "<class 'torch.Tensor'>\n",
      "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# collate stacks the tuples xs and ys of a batch into single tensors \n",
    "xx, yy = collate(small_ds)\n",
    "print(len(xx),len(yy))\n",
    "print(type(yy))\n",
    "print(yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a random sampler for the training set, and a non-random sampler for the validation set\n",
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data loaders\n",
    "# sampler and collate are the keywords, train_samp and collate are the values\n",
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a batch from the validation data and show the first image\n",
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw a random batch from the training data\n",
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw another random batch from the training data\n",
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2991, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model and train it for a single epoch\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "# loss function and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch to build data loaders\n",
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random batch from train_dl\n",
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1153, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a training loop\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's defaults work fine for most things however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the last (partial) batch?\n",
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random batch\n",
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0907, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a training loop\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "# report loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # training phase: handle batchnorm / dropout\n",
    "        model.train()\n",
    "        # print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # inference phase\n",
    "        model.eval()\n",
    "        # print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        # number of samples in validation batch\n",
    "        n_valid = len(valid_dl)\n",
    "        avg_loss, avg_acc = tot_loss/n_valid, tot_acc/n_valid\n",
    "        print(epoch, avg_loss, avg_acc)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: Are these validation results correct if batch size varies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dls` returns dataloaders for the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# note double batch size for validation data set, since it needs less storage because it has no gradients \n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2517) tensor(0.9162)\n",
      "1 tensor(0.3384) tensor(0.9004)\n",
      "2 tensor(0.1204) tensor(0.9655)\n",
      "3 tensor(0.1078) tensor(0.9684)\n",
      "4 tensor(0.1784) tensor(0.9442)\n"
     ]
    }
   ],
   "source": [
    "# make the data loaders\n",
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "# instantiate model and optimizer\n",
    "model,opt = get_model()\n",
    "# run the model for 5 epochs, compute loss and accuracy\n",
    "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training_jcat.ipynb to exp\\nb_03.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training_jcat.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
