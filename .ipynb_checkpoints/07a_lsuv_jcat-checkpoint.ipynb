{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layerwise Sequential Unit Variance (LSUV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_07 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the MNIST data and prepare to build a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 11 video](https://course.fast.ai/videos/?lesson=11&t=235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and validation data\n",
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "\n",
    "# normalize validation data to training data\n",
    "x_train,x_valid = normalize_to(x_train,x_valid)\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "\n",
    "# number of hidden layers, batch size\n",
    "# note -- we don't need nh for this notebook\n",
    "nh,bs = 50,512\n",
    "\n",
    "\n",
    "# number of digit classes \n",
    "c = y_train.max().item()+1\n",
    "\n",
    "# specify loss function\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "# create a DataBunch\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback function to resize flattened image back into a 2D array\n",
    "mnist_view = view_tfm(1,28,28)\n",
    "\n",
    "# list of callback functions\n",
    "cbfs = [Recorder,\n",
    "        partial(AvgStatsCallback,accuracy),\n",
    "        CudaCallback,\n",
    "        partial(BatchTransformXCallback, mnist_view)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list number of channels for each desired convolution layer\n",
    "nfs = [8,16,32,64,64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review documentation from the fastai library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-6a9cf9ddb02c>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-6a9cf9ddb02c>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Docstring: <no docstring>\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conv_layer??\n",
    "Signature: conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs)\n",
    "Docstring: <no docstring>\n",
    "Source:   \n",
    "def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):\n",
    "    layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),\n",
    "              GeneralRelu(**kwargs)]\n",
    "    if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))\n",
    "    return nn.Sequential(*layers)\n",
    "File:      c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_07.py\n",
    "Type:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??\n",
    "Init signature: nn.Sequential(*args)\n",
    "Source:        \n",
    "class Sequential(Module):\n",
    "    r\"\"\"A sequential container.\n",
    "    Modules will be added to it in the order they are passed in the constructor.\n",
    "    Alternatively, an ordered dict of modules can also be passed in.\n",
    "\n",
    "    To make it easier to understand, here is a small example::\n",
    "\n",
    "        # Example of using Sequential\n",
    "        model = nn.Sequential(\n",
    "                  nn.Conv2d(1,20,5),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(20,64,5),\n",
    "                  nn.ReLU()\n",
    "                )\n",
    "\n",
    "        # Example of using Sequential with OrderedDict\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                  ('conv1', nn.Conv2d(1,20,5)),\n",
    "                  ('relu1', nn.ReLU()),\n",
    "                  ('conv2', nn.Conv2d(20,64,5)),\n",
    "                  ('relu2', nn.ReLU())\n",
    "                ]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)\n",
    "        else:\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "\n",
    "    def _get_item_by_idx(self, iterator, idx):\n",
    "        \"\"\"Get the idx-th item of the iterator\"\"\"\n",
    "        size = len(self)\n",
    "        idx = operator.index(idx)\n",
    "        if not -size <= idx < size:\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        idx %= size\n",
    "        return next(islice(iterator, idx, None))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n",
    "        else:\n",
    "            return self._get_item_by_idx(self._modules.values(), idx)\n",
    "\n",
    "    def __setitem__(self, idx, module):\n",
    "        key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "        return setattr(self, key, module)\n",
    "\n",
    "    def __delitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            for key in list(self._modules.keys())[idx]:\n",
    "                delattr(self, key)\n",
    "        else:\n",
    "            key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "            delattr(self, key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._modules)\n",
    "\n",
    "    def __dir__(self):\n",
    "        keys = super(Sequential, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    def forward(self, input):\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input\n",
    "File:           c:\\users\\cross-entropy\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\container.py\n",
    "Type:           type\n",
    "Subclasses:     ConvReLU2d, LinearReLU, ConvBn2d, ConvBnReLU2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeneralRelu??\n",
    "Init signature: GeneralRelu(leak=None, sub=None, maxv=None)\n",
    "Docstring:     \n",
    "Base class for all neural network modules.\n",
    "\n",
    "Your models should also subclass this class.\n",
    "\n",
    "Modules can also contain other Modules, allowing to nest them in\n",
    "a tree structure. You can assign the submodules as regular attributes::\n",
    "\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            return F.relu(self.conv2(x))\n",
    "\n",
    "Submodules assigned in this way will be registered, and will have their\n",
    "parameters converted too when you call :meth:`to`, etc.\n",
    "Source:        \n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "File:           c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_06.py\n",
    "Type:           type\n",
    "Subclasses:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_learn_run??\n",
    "\n",
    "Signature:\n",
    "get_learn_run(\n",
    "    nfs,\n",
    "    data,\n",
    "    lr,\n",
    "    layer,\n",
    "    cbs=None,\n",
    "    opt_func=None,\n",
    "    uniform=False,\n",
    "    **kwargs,\n",
    ")\n",
    "Docstring: <no docstring>\n",
    "Source:   \n",
    "def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):\n",
    "    model = get_cnn_model(data, nfs, layer, **kwargs)\n",
    "    init_cnn(model, uniform=uniform)\n",
    "    return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)\n",
    "File:      c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_07.py\n",
    "Type:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cnn_model??\n",
    "\n",
    "Signature: get_cnn_model(data, nfs, layer, **kwargs)\n",
    "Docstring: <no docstring>\n",
    "Source:   \n",
    "def get_cnn_model(data, nfs, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))\n",
    "File:      c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_06.py\n",
    "Type:      function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_order',\n",
       " 'after_batch',\n",
       " 'begin_fit',\n",
       " 'name',\n",
       " 'plot',\n",
       " 'plot_loss',\n",
       " 'plot_lr',\n",
       " 'set_runner']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(cbfs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build  a Baseline CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)\n",
    "        self.relu = GeneralRelu(sub=sub, **kwargs)\n",
    "    \n",
    "    def forward(self, x): return self.relu(self.conv(x))\n",
    "    \n",
    "    @property\n",
    "    def bias(self): return -self.relu.sub\n",
    "    @bias.setter\n",
    "    def bias(self,v): self.relu.sub = -v\n",
    "    @property\n",
    "    def weight(self): return self.conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for training\n",
    "learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvLayer(\n",
       "    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (1): ConvLayer(\n",
       "    (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (2): ConvLayer(\n",
       "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (3): ConvLayer(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (4): ConvLayer(\n",
       "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (relu): GeneralRelu()\n",
       "  )\n",
       "  (5): AdaptiveAvgPool2d(output_size=1)\n",
       "  (6): Lambda()\n",
       "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to look at the paper [All You Need is a Good Init](https://arxiv.org/pdf/1511.06422.pdf), which introduces *Layer-wise Sequential Unit-Variance* (*LSUV*). We initialize our neural net with the usual technique, then we pass a batch through the model and check the outputs of the linear and convolutional layers. We can then rescale the weights according to the actual variance we observe on the activations, and subtract the mean we observe from the initial bias. That way we will have activations that stay normalized.\n",
    "\n",
    "We repeat this process until we are satisfied with the mean/variance we observe.\n",
    "\n",
    "Let's start by looking at a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [2.25954703125, tensor(0.1944, device='cuda:0')]\n",
      "valid: [2.17236640625, tensor(0.2360, device='cuda:0')]\n",
      "train: [1.162090625, tensor(0.6067, device='cuda:0')]\n",
      "valid: [0.2355705078125, tensor(0.9269, device='cuda:0')]\n",
      "Wall time: 2.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Layer-wise sequential Unit Variance (LSUV)\n",
    "Now we recreate our model and we'll try again with LSUV. Hopefully, we'll get better results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up for training again\n",
    "learn,run = get_learn_run(nfs, data, 0.6, ConvLayer, cbs=cbfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### break from code to check some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.avg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.batch_transform_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why do run.model, run.data, run.loss_func,run.opt return AttributeError: 'Runner' object has no attribute 'learn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.all_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs[0].set_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Callback.set_runner??\n",
    "Signature: Callback.set_runner(self, run)\n",
    "Docstring: <no docstring>\n",
    "Source:        def set_runner(self, run): self.run=run\n",
    "File:      c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_05b.py\n",
    "Type:      function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs[0].set_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run??\n",
    "Signature:   run(cb_name)\n",
    "Type:        Runner\n",
    "String form: <exp.nb_05b.Runner object at 0x000002A0D70316A0>\n",
    "File:        c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_05b.py\n",
    "Source:     \n",
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "    # Question: how does run have access to the Learner object?\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        try:\n",
    "            self.xb,self.yb = xb,yb\n",
    "            self('begin_batch')\n",
    "            self.pred = self.model(self.xb)\n",
    "            self('after_pred')\n",
    "            self.loss = self.loss_func(self.pred, self.yb)\n",
    "            self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backward()\n",
    "            self('after_backward')\n",
    "            self.opt.step()\n",
    "            self('after_step')\n",
    "            self.opt.zero_grad()\n",
    "        except CancelBatchException: self('after_cancel_batch')\n",
    "        finally: self('after_batch')\n",
    "\n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        try:\n",
    "            for xb,yb in dl: self.one_batch(xb, yb)\n",
    "        except CancelEpochException: self('after_cancel_epoch')\n",
    "\n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs,self.learn,self.loss = epochs,learn,tensor(0.) \n",
    "\n",
    "        try:\n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            self('begin_fit')\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                self('after_epoch')\n",
    "\n",
    "        except CancelTrainException: self('after_cancel_train')\n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, cb_name):\n",
    "        res = False\n",
    "        # Question: won't the following line always return res = False?\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### continue with code\n",
    "Helper function to get one batch of a given dataloader, with the callbacks called to preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(dl, run):\n",
    "    run.xb,run.yb = next(iter(dl))\n",
    "    # associat each callback with current Runner object\n",
    "    for cb in run.cbs: cb.set_runner(run)\n",
    "    run('begin_batch')\n",
    "    # return a batch\n",
    "    return run.xb,run.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a batch\n",
    "xb,yb = get_batch(data.train_dl, run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 28, 28])\n",
      "tensor([-0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245,\n",
      "        -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245,\n",
      "        -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245, -0.4245,\n",
      "        -0.4245, -0.4245, -0.4245, -0.4245], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(xb.size())\n",
    "print(xb[0,0,0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want the outputs of convolutional or linear layers. To find them, we need a recursive function. We can use `sum(list, [])` to concatenate the lists the function finds (`sum` applies the + operate between the elements of the list you pass it, beginning with the initial state in the second argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_modules(m, cond):\n",
    "    if cond(m): return [m]\n",
    "    return sum([find_modules(o,cond) for o in m.children()], [])\n",
    "\n",
    "def is_lin_layer(l):\n",
    "    lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)\n",
    "    return isinstance(l, lin_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mods = find_modules(learn.model, lambda o: isinstance(o,ConvLayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ConvLayer(\n",
       "   (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " ), ConvLayer(\n",
       "   (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "   (relu): GeneralRelu()\n",
       " )]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to grab the mean and std of the output of a hooked layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stat(hook, mod, inp, outp):\n",
    "    d = outp.data\n",
    "    hook.mean,hook.std = d.mean().item(),d.std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put model on the GPU\n",
    "mdl = learn.model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break from code to look up some docs in the fastai library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backend',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_construct',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_item_by_idx',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tracing_name',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-7c419205f275>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-7c419205f275>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    String form:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mdl??\n",
    "Signature:   mdl(*input, **kwargs)\n",
    "Type:        Sequential\n",
    "String form:\n",
    "Sequential(\n",
    "  (0): ConvLayer(\n",
    "    (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "    (relu): GeneralRelu()\n",
    "  )\n",
    "  (1): ConvLayer(\n",
    "    (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (relu): GeneralRelu()\n",
    "  )\n",
    "  (2): ConvLayer(\n",
    "    (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (relu): GeneralRelu()\n",
    "  )\n",
    "  (3): ConvLayer(\n",
    "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (relu): GeneralRelu()\n",
    "  )\n",
    "  (4): ConvLayer(\n",
    "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "    (relu): GeneralRelu()\n",
    "  )\n",
    "  (5): AdaptiveAvgPool2d(output_size=1)\n",
    "  (6): Lambda()\n",
    "  (7): Linear(in_features=64, out_features=10, bias=True)\n",
    ")\n",
    "Length:      8\n",
    "File:        c:\\users\\cross-entropy\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\container.py\n",
    "Source:     \n",
    "class Sequential(Module):\n",
    "    r\"\"\"A sequential container.\n",
    "    Modules will be added to it in the order they are passed in the constructor.\n",
    "    Alternatively, an ordered dict of modules can also be passed in.\n",
    "\n",
    "    To make it easier to understand, here is a small example::\n",
    "\n",
    "        # Example of using Sequential\n",
    "        model = nn.Sequential(\n",
    "                  nn.Conv2d(1,20,5),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(20,64,5),\n",
    "                  nn.ReLU()\n",
    "                )\n",
    "\n",
    "        # Example of using Sequential with OrderedDict\n",
    "        model = nn.Sequential(OrderedDict([\n",
    "                  ('conv1', nn.Conv2d(1,20,5)),\n",
    "                  ('relu1', nn.ReLU()),\n",
    "                  ('conv2', nn.Conv2d(20,64,5)),\n",
    "                  ('relu2', nn.ReLU())\n",
    "                ]))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(Sequential, self).__init__()\n",
    "        if len(args) == 1 and isinstance(args[0], OrderedDict):\n",
    "            for key, module in args[0].items():\n",
    "                self.add_module(key, module)\n",
    "        else:\n",
    "            for idx, module in enumerate(args):\n",
    "                self.add_module(str(idx), module)\n",
    "\n",
    "    def _get_item_by_idx(self, iterator, idx):\n",
    "        \"\"\"Get the idx-th item of the iterator\"\"\"\n",
    "        size = len(self)\n",
    "        idx = operator.index(idx)\n",
    "        if not -size <= idx < size:\n",
    "            raise IndexError('index {} is out of range'.format(idx))\n",
    "        idx %= size\n",
    "        return next(islice(iterator, idx, None))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n",
    "        else:\n",
    "            return self._get_item_by_idx(self._modules.values(), idx)\n",
    "\n",
    "    def __setitem__(self, idx, module):\n",
    "        key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "        return setattr(self, key, module)\n",
    "\n",
    "    def __delitem__(self, idx):\n",
    "        if isinstance(idx, slice):\n",
    "            for key in list(self._modules.keys())[idx]:\n",
    "                delattr(self, key)\n",
    "        else:\n",
    "            key = self._get_item_by_idx(self._modules.keys(), idx)\n",
    "            delattr(self, key)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._modules)\n",
    "\n",
    "    def __dir__(self):\n",
    "        keys = super(Sequential, self).__dir__()\n",
    "        keys = [key for key in keys if not key.isdigit()]\n",
    "        return keys\n",
    "\n",
    "    def forward(self, input):\n",
    "        for module in self._modules.values():\n",
    "            input = module(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-4892534782b4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-4892534782b4>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Init signature: Hooks(ms, f)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Hooks??\n",
    "Init signature: Hooks(ms, f)\n",
    "Docstring:      <no docstring>\n",
    "Source:        \n",
    "class Hooks(ListContainer):\n",
    "    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "\n",
    "    def remove(self):\n",
    "        for h in self: h.remove()\n",
    "File:           c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_06.py\n",
    "Type:           type\n",
    "Subclasses:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-fc9f468f00bf>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-28-fc9f468f00bf>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    Source:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# look up register_forward_hook \n",
    "learn.model.register_forward_hook??\n",
    "Signature: learn.model.register_forward_hook(hook)\n",
    "Source:   \n",
    "    def register_forward_hook(self, hook):\n",
    "        r\"\"\"Registers a forward hook on the module.\n",
    "\n",
    "        The hook will be called every time after :func:`forward` has computed an output.\n",
    "        It should have the following signature::\n",
    "\n",
    "            hook(module, input, output) -> None or modified output\n",
    "\n",
    "        The hook can modify the output. It can modify the input inplace but\n",
    "        it will not have effect on forward since this is called after\n",
    "        :func:`forward` is called.\n",
    "\n",
    "        Returns:\n",
    "            :class:`torch.utils.hooks.RemovableHandle`:\n",
    "                a handle that can be used to remove the added hook by calling\n",
    "                ``handle.remove()``\n",
    "        \"\"\"\n",
    "        handle = hooks.RemovableHandle(self._forward_hooks)\n",
    "        self._forward_hooks[handle.id] = hook\n",
    "        return handle\n",
    "File:      c:\\users\\cross-entropy\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
    "Type:      method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-fd027b1fa91b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-fd027b1fa91b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Init signature: Hook(m, f)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Hook??\n",
    "Init signature: Hook(m, f)\n",
    "Docstring:      <no docstring>\n",
    "Source:        \n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "File:           c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_06.py\n",
    "Type:           type\n",
    "Subclasses:     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-21a494bc87a4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-30-21a494bc87a4>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Init signature: ListContainer(items)\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "ListContainer??\n",
    "Init signature: ListContainer(items)\n",
    "Docstring:      <no docstring>\n",
    "Source:        \n",
    "class ListContainer():\n",
    "    def __init__(self, items): self.items = listify(items)\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, (int,slice)): return self.items[idx]\n",
    "        if isinstance(idx[0],bool):\n",
    "            assert len(idx)==len(self) # bool mask\n",
    "            return [o for m,o in zip(idx,self.items) if m]\n",
    "        return [self.items[i] for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res\n",
    "File:           c:\\users\\cross-entropy\\fastai\\course-v3\\nbs\\dl2\\exp\\nb_06.py\n",
    "Type:           type\n",
    "Subclasses:     Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  resume with the code\n",
    "So now we can look at the mean and std of the conv layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hook' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-b4ed974ce35e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Hook' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "# train a batch\n",
    "with Hooks(mods, append_stat) as hooks:\n",
    "    # forward pass\n",
    "    mdl(xb)\n",
    "    for hook in hooks: print(hook.mean,hook.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first adjust the bias terms to make the means 0, then we adjust the standard deviations to make the stds 1 (with a threshold of 1e-3). The `mdl(xb) is not None` clause is just there to pass `xb` through `mdl` and compute all the activations so that the hooks get updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0373, -0.1247, -0.0927, -0.1047,  0.1005, -0.0499, -0.1082,  0.0650,\n",
       "         0.0426, -0.0307], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model[7].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: switching the order, i.e. scaling weights by variance first, then the removing the mean from the bias achieves near-perfect normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# LSUV algorithm\n",
    "def lsuv_module(m, xb):\n",
    "    h = Hook(m, append_stat)\n",
    "    while mdl(xb) is not None and abs(h.std-1) > 1e-3: m.weight.data /= h.std\n",
    "    while mdl(xb) is not None and abs(h.mean)  > 1e-3: m.bias -= h.mean\n",
    "\n",
    "    h.remove()\n",
    "    return h.mean,h.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute that initialization on all the conv layers in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.0035475561664953e-08, 0.9999998807907104)\n",
      "(1.1099844599016251e-08, 0.9999998807907104)\n",
      "(-2.0139850676059723e-08, 1.0)\n",
      "(8.149072527885437e-09, 1.0)\n",
      "(-1.4901161193847656e-08, 0.9999999403953552)\n"
     ]
    }
   ],
   "source": [
    "for m in mods: print(lsuv_module(m, xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean doesn't exactly stay at 0. since we change the standard deviation after by scaling the weight.\n",
    "Note from jcat: But if you modify the code to scale the standard deviations first, then correct the mean, you achieve near-perfect normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then training is beginning on better grounds, and we get impoved accuracy after 2 epochs compared to the initial run with LSUV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.4852893359375, tensor(0.8433, device='cuda:0')]\n",
      "valid: [0.18036103515625, tensor(0.9479, device='cuda:0')]\n",
      "train: [0.112610205078125, tensor(0.9645, device='cuda:0')]\n",
      "valid: [0.09935478515625, tensor(0.9698, device='cuda:0')]\n",
      "Wall time: 2.83 s\n"
     ]
    }
   ],
   "source": [
    "# train with LSUV\n",
    "%time run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSUV is particularly useful for more complex and deeper architectures that are hard to initialize to get unit variance at the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python notebook2script.py 07a_lsuv.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
