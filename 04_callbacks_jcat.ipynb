{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all code from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_03 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST training and validation data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=4799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_valid,y_valid = get_data()\n",
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "nh,bs = 50,64\n",
    "c = y_train.max().item()+1\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 784])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# data set properties x and y come from Dataset\n",
    "print(valid_ds.x.shape)\n",
    "print(valid_ds.y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Improving the fit() function\n",
    "### Factor the connected pieces of information (model, optimizer, loss function and data) from the `fit()` argument list into a `Learner Class object`\n",
    "\n",
    "`fit(n_epochs, model, loss_func, opt, train_dl, valid_dl)`\n",
    "\n",
    "Let's modify the call to `fit()` to look like this:\n",
    "\n",
    "`fit(n_epochs, learner)`\n",
    "\n",
    "Here, `learner` is a `Learner` Class object, that we will define as a container for `model`, `loss_func`, `opt`, `train_dl`, `valid_dl` to be passed into `fit()`\n",
    "\n",
    "This will allow us to tweak what's happening inside the training loop in other places of the code. Because the `Learner` Class object will be mutable, changing any of its attributes elsewhere will modify our training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5363)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataBunch class\n",
    "For convenience we define a `DataBunch()` class, as a container for the DataLoader() objects and optionally n_out, the number of output channels.\n",
    "DataBunch() takes inputs train_dl(), valid_dl(), and optionally n_out.\n",
    "It has properties train_ds(), valid_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, n_out=None):\n",
    "        self.train_dl,self.valid_dl,self.n_out = train_dl,valid_dl,n_out\n",
    "    \n",
    "    # add train_ds() as an attribute\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "     \n",
    "        \n",
    "    # add valid_ds() as an attribute\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Learner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# get_model() instantiates the model and the optimizer\n",
    "def get_model(data, learning_rate=0.5, n_hidden = 50):\n",
    "    n_columns = data.train_ds.x.shape[1]\n",
    "    n_out = data.n_out\n",
    "    model = nn.Sequential(nn.Linear(n_columns,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out))\n",
    "    return model, optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# the Learner() class is a container for the model, optimization, loss function and data \n",
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the fit() function according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor the fit() function according to our plan\n",
    "#     Note the order of the inputs has been switched in order to allow n_epochs to be a keyword argument\n",
    "def fit(learn,n_epochs):\n",
    "    \n",
    "    # n_epochs is a keyword argument\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # training phase\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            pred = learn.model(xb)\n",
    "            loss = learn.loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "\n",
    "        # prediction phase\n",
    "        # tells pytorch that we are in eval phase, so no dropout, batchnorm, etc.\n",
    "        learn.model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb)\n",
    "                tot_acc  += accuracy(pred,yb)\n",
    "        \n",
    "        # compute average loss and accuracy\n",
    "        n_valid = len(learn.data.valid_dl)\n",
    "        avg_loss, avg_acc = tot_loss/n_valid, tot_acc/n_valid\n",
    "        print(epoch, avg_loss ,avg_acc )\n",
    "        \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for minibatch training package up the data into a DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 784])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# package up the data in a DataBunch object\n",
    "# the * forces you to generate and process the entire sample\n",
    "batch_size = 64\n",
    "n_out = 10\n",
    "data = DataBunch(*get_dls(train_ds, valid_ds, batch_size=batch_size), n_out=n_out)\n",
    "\n",
    "# data also has properties x and y, inherited from train_ds\n",
    "print(data.train_dl.dataset.x.shape)\n",
    "print(data.train_dl.dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Learner object\n",
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check module list outputs\n",
    "print(learn.model.train())\n",
    "print(learn.model.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a minibatch training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2380) tensor(0.9299)\n"
     ]
    }
   ],
   "source": [
    "# instantiate a minibatch training loop and start training!\n",
    "loss,acc = fit(learn, n_epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding callbacks \n",
    "Callbacks are functions that can be set to initiate desired sequences of actions at various stages in the minibatch training process. They will allow us to simplify the training loop, and provide added flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our training loop (without validation) from the previous notebook, with the inner loop contents factored out:\n",
    "\n",
    "```python\n",
    "def one_batch(xb,yb):\n",
    "    pred = model(xb)\n",
    "    loss = loss_func(pred, yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    \n",
    "def fit():\n",
    "    for epoch in range(n_epochs):\n",
    "        for b in train_dl: one_batch(*b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5628)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CallBack() Class\n",
    "We define a set of nine callbacks, associated with various well-defined stages in the training process. \n",
    "\n",
    "Note that each callback returns `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback():\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn = learn\n",
    "        return True\n",
    "    def after_fit(self):\n",
    "        return True\n",
    "    def begin_epoch(self, epoch):\n",
    "        self.epoch=epoch\n",
    "        return True\n",
    "    def begin_validate(self):\n",
    "        return True\n",
    "    def after_epoch(self): \n",
    "        return True\n",
    "    def begin_batch(self, xb, yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        return True\n",
    "    def after_loss(self, loss):\n",
    "        self.loss = loss\n",
    "        return True\n",
    "    def after_backward(self):\n",
    "        return True\n",
    "    def after_step(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CallbackHandler Class\n",
    "Collects together a set of methods that, given a list of callbacks, sets a flag for each callback that is included in the list. Adds a method do_stop() that returns the `learn.stop` flag if it exists, otherwise sets `learn.stop` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackHandler():\n",
    "    def __init__(self,callbacks=None):\n",
    "        self.callbacks = callbacks if callbacks else []\n",
    "\n",
    "    def begin_fit(self, learn):\n",
    "        self.learn,self.in_train = learn,True\n",
    "        learn.stop = False\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.begin_fit(learn)\n",
    "        return state\n",
    "\n",
    "    def after_fit(self):\n",
    "        state = not self.in_train\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.after_fit()\n",
    "        return state\n",
    "    \n",
    "    def begin_epoch(self, epoch):\n",
    "        # what does the next line do?\n",
    "        self.learn.model.train()\n",
    "        self.in_train=True\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.begin_epoch(epoch)\n",
    "        return state\n",
    "\n",
    "    def begin_validate(self):\n",
    "        # what does the next line do?\n",
    "        self.learn.model.eval()\n",
    "        self.in_train=False\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.begin_validate()\n",
    "        return state\n",
    "\n",
    "    def after_epoch(self):\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.after_epoch()\n",
    "        return state\n",
    "    \n",
    "    def begin_batch(self, xb, yb):\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.begin_batch(xb, yb)\n",
    "        return state\n",
    "\n",
    "    def after_loss(self, loss):\n",
    "        state = self.in_train\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.after_loss(loss)\n",
    "        return state\n",
    "\n",
    "    def after_backward(self):\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.after_backward()\n",
    "        return state\n",
    "\n",
    "    def after_step(self):\n",
    "        state = True\n",
    "        for callback in self.callbacks: \n",
    "            state = state and callback.after_step()\n",
    "        return state\n",
    "    \n",
    "    def do_stop(self):\n",
    "        try:     \n",
    "            return self.learn.stop\n",
    "        finally: \n",
    "            self.learn.stop = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A TestCallback Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "class TestCallback(Callback):\n",
    "    # modify the begin_fit() callback by adding an iteration counter\n",
    "    def begin_fit(self,learn):\n",
    "        # ????? not sure why we need to invoke super() below\n",
    "        super().begin_fit(learn)\n",
    "        self.n_iters = 0\n",
    "        self.n_epochs_float = 0.\n",
    "        return True\n",
    "    # modify the afer_step() callback to increment the iteration counter, print the current iteration number,\n",
    "    #   and to set the learn.stop flag to True after max_iter iterations\n",
    "    # why doesn't after_step() inherit from its previous definition, as we did with begin_fit()?\n",
    "    def after_step(self):\n",
    "        self.n_iters += 1\n",
    "        print(self.n_iters)\n",
    "        if self.n_iters>=max_iter: \n",
    "            self.learn.stop = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored training loop, with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process a single batch, with an input list of callbacks\n",
    "def one_batch(xb, yb, callback):\n",
    "    \n",
    "    # run the model and computer the loss function for the current batch\n",
    "    if not callback.begin_batch(xb,yb): \n",
    "        return\n",
    "    loss = callback.learn.loss_func(callback.learn.model(xb), yb)\n",
    "    \n",
    "    # do backpropagation for the current batch\n",
    "    if not callback.after_loss(loss): \n",
    "        return\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameters for the current batch\n",
    "    if callback.after_backward(): \n",
    "        callback.learn.opt.step()\n",
    "        \n",
    "    # zero the gradients to prepare for the next batch\n",
    "    if callback.after_step(): \n",
    "        callback.learn.opt.zero_grad()\n",
    "\n",
    "# process the entire dataset, with an input list of callbacks\n",
    "#   (i.e. loop through all the batches)\n",
    "def all_batches(dataloader, callback):\n",
    "    for xb,yb in dataloader:\n",
    "        \n",
    "        # process a batch, then check whether to stop or to process the next batch\n",
    "        one_batch(xb, yb, callback)\n",
    "        if callback.do_stop(): \n",
    "            return\n",
    "\n",
    "# training loop with an input list of callbacks\n",
    "def fit(learn, callback, n_epochs):\n",
    "    \n",
    "    # check whether or not to start the training loop\n",
    "    if not callback.begin_fit(learn): \n",
    "        return\n",
    "    \n",
    "    # loop over the specified number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # check whether or not to process the next epoch\n",
    "        if not callback.begin_epoch(epoch): \n",
    "            continue # jumps to next epoch\n",
    "        all_batches(learn.data.train_dl, callback)\n",
    "        \n",
    "        # check whether or not to process the validation set\n",
    "        if callback.begin_validate():\n",
    "            with torch.no_grad(): \n",
    "                all_batches(learn.data.valid_dl, callback)\n",
    "        \n",
    "        # check whether or not to break after the epoch has been processed\n",
    "        if callback.do_stop() or not callback.after_epoch(): \n",
    "            break\n",
    "            \n",
    "    # set the callback state to indicate that fit() has been run\n",
    "    callback.after_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CallbackHandler at 0x253001706d8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CallbackHandler([TestCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# run a training loop with TestCallback\n",
    "fit(learn, callback=CallbackHandler([TestCallback()]),n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is roughly how fastai does it now (except that the handler can also change and return `xb`, `yb`, and `loss`). But let's see if we can make things simpler and more flexible, so that a single class has access to everything and can change anything at any time. The fact that we're passing the `callback` list to so many functions is a strong hint they should all be in the same class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump_to lesson 9 video](https://course.fast.ai/videos/?lesson=9&t=5811)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the Callback() class\n",
    "add a `name` property, `_order` and `run` attributes, and `set` and `get` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "\n",
    "# use regular expressions to construct a 'snake case' callback name for each 'camel case' callback name\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "# create a Callback() class\n",
    "class Callback():\n",
    "    # initialize _order to zero. \n",
    "    #     ????? not sure why we need _order \n",
    "    _order=0\n",
    "    def set_runner(self, run): \n",
    "        self.run=run\n",
    "    def __getattr__(self, callback_name): \n",
    "        return getattr(self.run, callback_name)\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        # if name does not exist, set the name to 'callback'\n",
    "        return camel2snake(name or 'callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class TrainEvalCallback()\n",
    "Switch the model back and forth in training or validation mode, maintain iteration count and fraction of the  elapsed iterations in the epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    \n",
    "    # initialize the epoch and iteration counters\n",
    "    def begin_fit(self):\n",
    "        # n_epochs_float keeps track of where we are in the current epoch, need not be integer\n",
    "        #     in original code, this variable was named n_epochs, but there is another variable with than name\n",
    "        #     so it is preferable to give it a more apporpriate and descriptive name.\n",
    "        self.run.n_epochs_float=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    # if we are in the training phase, increment the epoch and iteration counters\n",
    "    def after_batch(self):\n",
    "        if not self.in_train: \n",
    "            return\n",
    "        # each training iteration represents a fraction of an epoch\n",
    "        #      n_iters comes from TestCallback, and is an attribute of begin_fit\n",
    "        self.run.n_epochs_float += 1./self.n_iters\n",
    "        self.run.n_iter   += 1\n",
    "    \n",
    "    # execute the training phase\n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs_float=self.n_epochs_float\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "    \n",
    "    # execute prediction phase\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also re-create our TestCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def after_step(self):\n",
    "        if self.train_eval.n_iters>=max_iter: \n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_eval_callback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train_eval'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a 'snake case' callback name given a 'camel case' callback name\n",
    "callback_name = 'TrainEvalCallback'\n",
    "print(camel2snake(callback_name))\n",
    "\n",
    "# trim the '_callback' suffix\n",
    "TrainEvalCallback().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import *\n",
    "\n",
    "# function to convert any input into a list\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Runner Class -- to be implemented in fastai v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    def __init__(self, callbacks=None, callback_funcs=None):\n",
    "        # create a list of callbacks from the input callbacks\n",
    "        callbacks = listify(callbacks)\n",
    "        # append to the callbacks list from the input list of callback_funcs\n",
    "        for callback_func in listify(callback_funcs):\n",
    "            callback = callback_func()\n",
    "            setattr(self, callback.name, callback)\n",
    "            callbacks.append(callback)\n",
    "        # set the stopping flag to `False` and append TrainEvalCallback() to the callbacks list\n",
    "        self.stop,self.callbacks = False,[TrainEvalCallback()]+callbacks\n",
    "\n",
    "    @property\n",
    "    def opt(self):       return self.learn.opt\n",
    "    @property\n",
    "    def model(self):     return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self):      return self.learn.data\n",
    "\n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        if self('begin_batch'): \n",
    "            return\n",
    "        # run the model\n",
    "        self.pred = self.model(self.xb)\n",
    "        # ????? no `after_pred` callback has been defined ?????\n",
    "        if self('after_pred'): \n",
    "            return\n",
    "        # compute the loss function\n",
    "        self.loss = self.loss_func(self.pred, self.yb)\n",
    "        # should `self('after_loss')` be `not self('after_loss')`, for consistency with previous version?\n",
    "        if self('after_loss') or not self.in_train: \n",
    "            return\n",
    "        # do backpropagation\n",
    "        self.loss.backward()\n",
    "        if self('after_backward'): \n",
    "            return\n",
    "        # update parameters\n",
    "        self.opt.step()\n",
    "        if self('after_step'): \n",
    "            return\n",
    "        # zero the gradients to prepare for the next batch\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "    def all_batches(self, dataloader):\n",
    "        self.n_iters = len(dataloader)\n",
    "        self.n_epochs_float = 0.\n",
    "        for xb,yb in dataloader:\n",
    "            # break if stopping flag has been set\n",
    "            if self.stop: \n",
    "                break\n",
    "            # process the next batch and set the `after_batch` flag\n",
    "            self.one_batch(xb, yb)\n",
    "            self('after_batch')\n",
    "        # set the stopping flag to `False`    \n",
    "        self.stop=False\n",
    "\n",
    "    def fit(self, learn, n_epochs):\n",
    "        self.n_epochs,self.learn = n_epochs,learn\n",
    "\n",
    "        try:\n",
    "            for callback in self.callbacks: \n",
    "                callback.set_runner(self)\n",
    "            if self('begin_fit'): \n",
    "                return\n",
    "            for epoch in range(n_epochs):\n",
    "                self.epoch = epoch\n",
    "                \n",
    "                # training phase\n",
    "                if not self('begin_epoch'): \n",
    "                    self.all_batches(self.data.train_dl)\n",
    "                \n",
    "                # validation phase\n",
    "                with torch.no_grad(): \n",
    "                    if not self('begin_validate'): \n",
    "                        self.all_batches(self.data.valid_dl)\n",
    "                # break if `after_epoch` state is `True`\n",
    "                if self('after_epoch'): \n",
    "                    break\n",
    "            \n",
    "        finally:\n",
    "            # set the `after_fit` state to `True`\n",
    "            self('after_fit')\n",
    "            # erase the learner object\n",
    "            self.learn = None\n",
    "\n",
    "    def __call__(self, callback_name):\n",
    "        for callback in sorted(self.callbacks, key=lambda x: x._order):\n",
    "            # getattr returns the callback name\n",
    "            f = getattr(callback, callback_name, None)\n",
    "            # return `True` if this callback has a name, otherwise return `False`\n",
    "            if f and f(): \n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AvgStatsCallback computes metrics and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): \n",
    "        self.metrics,self.in_train = listify(metrics),in_train\n",
    "    \n",
    "    # initialize total_loss, count, and total_metrics\n",
    "    def reset(self):\n",
    "        self.total_loss,self.count = 0.,0\n",
    "        self.total_metrics = [0.] * len(self.metrics)\n",
    "        \n",
    "    # combine loss and metrics\n",
    "    @property\n",
    "    def all_stats(self): \n",
    "        return [self.total_loss.item()] + self.total_metrics\n",
    "    \n",
    "    # compute avg loss and metrics\n",
    "    @property\n",
    "    def avg_stats(self): \n",
    "        return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    # compute and display stats\n",
    "    def __repr__(self):\n",
    "        if not self.count: \n",
    "            return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, run):\n",
    "        # get the number of samples in this batch\n",
    "        n_samples_in_batch = run.xb.shape[0]\n",
    "        # weight the loss function for the batch by the number of samples in the batch\n",
    "        self.total_loss += run.loss * n_samples_in_batch\n",
    "        # accumulate count of samples processed\n",
    "        self.count += n_samples_in_batch\n",
    "        # accumulate the metrics\n",
    "        for i,metric in enumerate(self.metrics):\n",
    "            self.total_metrics[i] += metric(run.pred, run.yb) * n_samples_in_batch\n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats,self.valid_stats = AvgStats(metrics,in_train=True),AvgStats(metrics,in_train=False)\n",
    "        \n",
    "    # initialize train_stats and valid_stats\n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "    # compute and accumulate stats\n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): \n",
    "            stats.accumulate(self.run)\n",
    "    # print stats\n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a learner object\n",
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set metric to accuracy\n",
    "stats = AvgStatsCallback([accuracy])\n",
    "# instantiate a Runner object\n",
    "run = Runner(callbacks=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.31608310546875, tensor(0.9035)]\n",
      "valid: [0.17542603759765624, tensor(0.9454)]\n",
      "train: [0.14302001953125, tensor(0.9573)]\n",
      "valid: [0.12961510009765626, tensor(0.9607)]\n"
     ]
    }
   ],
   "source": [
    "# run a training loop\n",
    "run.fit(learn, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12961510009765626, tensor(0.9607))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute stats\n",
    "loss,acc = stats.valid_stats.avg_stats\n",
    "assert acc>0.9\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_callback_func = partial(AvgStatsCallback,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(callback_funcs=acc_callback_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.108138134765625, tensor(0.9671)]\n",
      "valid: [0.11042103271484376, tensor(0.9669)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(learn, n_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Jupyter means we can get tab-completion even for dynamic code like this! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.11042103271484376, tensor(0.9669)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.avg_stats.valid_stats.avg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valid: [0.11042103271484376, tensor(0.9669)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.avg_stats.valid_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 04_callbacks_jcat.ipynb to exp\\nb_04.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 04_callbacks_jcat.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
