{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02 import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set matplotlib color map\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation sets\n",
    "x_train,y_train,x_valid,y_valid = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows and columns of training data\n",
    "n,m = x_train.shape\n",
    "# number of possible output labels\n",
    "c = y_train.max()+1\n",
    "# size of the hidden layer\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple fully connected network\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out)]\n",
    "    # sequentially apply layers and return the output activations\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = Model(n_in = m, n_hidden = nh, n_out = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01087 ,  0.029903,  0.056536, -0.153222,  0.121045, -0.158346, -0.026219,  0.026577, -0.021856,  0.025388],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict training labels\n",
    "pred = model(x_train)\n",
    "\n",
    "# look at the output activations for the first training example\n",
    "import numpy as np\n",
    "pred.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss is the usual loss function for multi-label classification problems. Since cross-entropy loss is computed from the probabilities of the predicted classes, we must first convert the output activations to probabilities; this is accomplished by applying the softmax function to the output activations.\n",
    "\n",
    "Softmax is defined by:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}}$$ \n",
    "\n",
    "In the above formulas, $i$ indexes the output activation node.\n",
    "\n",
    "In practice, we will need the log of the softmax when we calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log(softmax)\n",
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "shape  torch.Size([50000, 10])\n",
      "\n",
      "output log probabilities:  [-2.358608 -2.31396  -2.219606 -2.459056 -2.124907 -2.414343 -2.240862 -2.208187 -2.401075 -2.336613]\n",
      "\n",
      "output probabilities:  [0.094552 0.098869 0.108652 0.085516 0.119444 0.089426 0.106367 0.1099   0.09062  0.096654]\n",
      "\n",
      "sum of probabilities is  0.99999994\n",
      "\n",
      "predicted digit is  4\n"
     ]
    }
   ],
   "source": [
    "# applying softmax to the output activations converts them to log probabilities\n",
    "sm_pred = log_softmax(pred)\n",
    "print('\\nshape ',sm_pred.shape)\n",
    "\n",
    "# look at the first example in the training set\n",
    "print('\\noutput log probabilities: ',sm_pred.detach().numpy()[1,:])\n",
    "print('\\noutput probabilities: ',np.exp(sm_pred.detach().numpy()[1,:]))\n",
    "\n",
    "# check that sum of probabilities is one\n",
    "print('\\nsum of probabilities is ',np.exp(sm_pred.detach().numpy()[1,:]).sum())\n",
    "\n",
    "# the predicted digit is the output with the highest probability\n",
    "print('\\npredicted digit is ',np.argmax(np.exp(sm_pred.detach().numpy()[1,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Loss\n",
    "The cross entropy loss for some target $x$ and some prediction $p(x)$ is given by:\n",
    "\n",
    "$$ \\ell = -\\sum x\\, \\log p(x)$$\n",
    "\n",
    "But since our $x$s are 1-hot encoded, this can be rewritten as \n",
    "\n",
    "$$ \\ell = -\\log(p_{i})$$\n",
    "where $i$ is the index of the true class label.\n",
    "\n",
    "And the loss function is the mean cross-entropy loss over all the examples:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{\\sum \\ell}{N}$$\n",
    "where $N$ is the number of examples in the training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the cross-entropy loss function using numpy-style [integer array indexing](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing). Note that PyTorch supports all the tricks in the advanced indexing methods discussed in that link. \n",
    "\n",
    "The next few cells provide examples of indexing, then we'll go on to implement the cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train (target) shape is  torch.Size([50000])\n",
      "\n",
      "sm_pred (input) shape is  torch.Size([50000, 10])\n"
     ]
    }
   ],
   "source": [
    "# shapes of training set predictions output and of the training labels\n",
    "print('\\ny_train (target) shape is ',y_train.shape)\n",
    "print('\\nsm_pred (input) shape is ',sm_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing, example 1\n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4554, -2.3586, -2.1839], grad_fn=<IndexBackward>)\n",
      "tensor(-2.4554, grad_fn=<SelectBackward>)\n",
      "tensor(-2.3586, grad_fn=<SelectBackward>)\n",
      "tensor(-2.1839, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# indexing, example 2\n",
    "print(sm_pred[[0,1,2], [5,0,4]]) \n",
    "print(sm_pred[0][5])\n",
    "print(sm_pred[1][0])\n",
    "print(sm_pred[2][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2862, -2.2671, -2.2405, -2.4503, -2.1760, -2.4554],\n",
      "        [-2.3586, -2.3140, -2.2196, -2.4591, -2.1249, -2.4143],\n",
      "        [-2.2770, -2.2899, -2.2871, -2.3123, -2.1839, -2.3962]],\n",
      "       grad_fn=<IndexBackward>)\n",
      "tensor([-2.2862, -2.2671, -2.2405, -2.4503, -2.1760, -2.4554],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([-2.3586, -2.3140, -2.2196, -2.4591, -2.1249, -2.4143],\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([-2.2770, -2.2899, -2.2871, -2.3123, -2.1839, -2.3962],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "# indexing, example 3\n",
    "print(sm_pred[[0,1,2], :6])\n",
    "print(sm_pred[0,:6])\n",
    "print(sm_pred[1,:6])\n",
    "print(sm_pred[2,:6])\n",
    "# print(sm_pred[[0,1,2], 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nll (negative log likelihood) loss function\n",
    "#    nll is our implementation of the cross-entropy loss function\n",
    "#    -- computes a vector of the predicted log probabilities corresponding to the true labels for each example\n",
    "#    -- outputs the negative mean of this vector, which is the cross-entropy loss function\n",
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3119, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the nll (or cross-entropy) loss function\n",
    "loss = nll(sm_pred, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula \n",
    "\n",
    "$$\\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$ \n",
    "\n",
    "gives a simplification when we compute the log softmax, which was previously defined as `(x.exp()/(x.exp().sum(-1,keepdim=True))).log()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor log_softmax\n",
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that nll using the refactored log_softmax is the same as before\n",
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a problem with the logsumexp function: overflow can occur if the exponents become large positive numbers. \n",
    "The [LogSumExp trick] provides a brilliant way to compute the log of the sum of exponentials in a more stable way. (https://en.wikipedia.org/wiki/LogSumExp). The idea is to express logsumexp in the following way:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "where $a$ is the maximum of the $x_{j}$.\n",
    "\n",
    "Note that because of the clever choice of $a$, all the exponents on the right-hand side are $\\le 0$. Recall that the exponential of a negative number is always a positive number that is less than $1$. So the expression on the right hand side contains no exponentials of large positive numbers. This solves the overflow problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LogSumExp trick for numerical stability\n",
    "def logsumexp(x):\n",
    "    # index (-1) indicates the last dimension, in this case, it's the same as (1), which is across rows\n",
    "    # but pytorch max returns a vector of values and also a vector of the column indices corresponding to those values \n",
    "    # the [0] at the end of the line below is necessary to select the values\n",
    "    m = x.max(-1)[0]\n",
    "    # m is a vector; to convert it to a column vector, i.e. a matrix with one column, use m[:,None] \n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence between our logsumexp function and python's implementation\n",
    "test_near(logsumexp(pred), pred.logsumexp(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use it for our `log_softmax` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor log_softmax again\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of nll and loss\n",
    "test_near(nll(log_softmax(pred), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation of nll_loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of pytorch's implementation of nll and loss\n",
    "test_near(F.nll_loss(F.log_softmax(pred, -1), y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `F.log_softmax` and `F.nll_loss` are combined in one optimized function, `F.cross_entropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify equivalence of python's corss-entropy and loss\n",
    "test_near(F.cross_entropy(pred, y_train), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are allowed to use Pytorch implementation of loss function\n",
    "#    inputs preds, labels\n",
    "#    where preds are the activation outputs\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy is the fraction of correct predictions\n",
    "# predicted class is torch.argmax(prediction_probs, dim=1)==target)\n",
    "def accuracy(prediction_probs, target): return (torch.argmax(prediction_probs, dim=1)==target).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0109,  0.0299,  0.0565, -0.1532,  0.1210, -0.1583, -0.0262,  0.0266,\n",
       "         -0.0219,  0.0254], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a mini-batch and run the model to get predictions\n",
    "bs=64                  # batch size\n",
    "xb = x_train[0:bs]     # a mini-batch from x\n",
    "preds = model(xb)      # predictions (activation outputs)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3103, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the loss function\n",
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy for a forward pass on a single batch\n",
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "lr = 0.5   # learning rate\n",
    "epochs = 1 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "# why doesn't it need inputs of model, epochs, bs, n, x_train, y_train\n",
    "def fit():\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches\n",
    "        # batch index goes from 0 to (n-1)/bs the greatest integer number of batches in n\n",
    "        # throwing away the remaining samples\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            # set_trace()\n",
    "            # index of first sample in batch number i \n",
    "            start_i = i*bs\n",
    "            # 1 + index of last sample in batch number i\n",
    "            end_i = start_i+bs\n",
    "            # get the batch and compute the loss function\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            # compute the gradients of the loss function\n",
    "            loss.backward()\n",
    "            with torch.no_grad(): # temporarily sets all the requires_grad flags (whose default values are True) to False\n",
    "                for l in model.layers:\n",
    "                    if hasattr(l, 'weight'):\n",
    "                        # update the weights and biases using SGD\n",
    "                        l.weight -= l.weight.grad * lr\n",
    "                        l.bias   -= l.bias.grad   * lr\n",
    "                        # zero the gradients\n",
    "                        l.weight.grad.zero_()\n",
    "                        l.bias  .grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "# a linear layer has weight and bias\n",
    "print(hasattr(nn.Linear(nh,c.numpy()),'weight'),hasattr(nn.Linear(nh,c.numpy()),'bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "# a ReLU layer has no parameters\n",
    "print(hasattr(nn.ReLU(),'weight'),hasattr(nn.ReLU(),'bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1621, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# loss function and accuracy after 1 epoch of training with bs = 64 and lr = 0.5\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using parameters and optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `nn.Module.__setattr__` and move relu to functional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original version of Model, the starting point for refactoring\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # inherits from nn.Module, and super().__init__() allows access to all the methods and attributes of nn.Module\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out)]\n",
    "    # sequentially apply layers and return the output activations\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternate implementation abandons the layers approach\n",
    "# what is meant by \"Use nn.Module.__setattr__ ?\"\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # inherits from nn.Module, and super().__init__() allows access to the methods and attributes of nn.Module\n",
    "        # if no arguments were passed, the line below would just be super() \n",
    "        super().__init__()\n",
    "        # implicitly using __setattr__ to set module attributes\n",
    "        self.l1 = nn.Linear(n_in,n_hidden)\n",
    "        self.l2 = nn.Linear(n_hidden,n_out)\n",
    "    # avoids use of loop\n",
    "    # why move ReLU (and not l1 and l2) to functional?\n",
    "    def __call__(self, x): \n",
    "        return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Model\n",
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1: Linear(in_features=784, out_features=50, bias=True)\n",
      "l2: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# model.named_children is a generator over child modules, returns name and module itself\n",
    "# children of self\n",
    "for name,l in model.named_children(): print(f\"{name}: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns named_children names and modules\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one of the children modules\n",
    "model.l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor parameter updates using model.parameters()\n",
    "# training loop\n",
    "def fit():\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            # compute loss function for this batch\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD and zero out the gradients\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1276, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# record the final loss and accuracy \n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, PyTorch overrides the `__setattr__` function in `nn.Module` so that the submodules you define are properly registered as parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class duplicates the functionality of Model without Pytorch\n",
    "# notice it does not inherit from a parent class\n",
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {} #dictionary\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "        \n",
    "    def __setattr__(self,k,v):\n",
    "        if not k.startswith(\"_\"): self._modules[k] = v\n",
    "        # what are k, v?\n",
    "        # what is the super class being inherited from?\n",
    "        super().__setattr__(k,v)\n",
    "        \n",
    "    def __repr__(self): return f'{self._modules}' # return the _modules dictionary\n",
    "    \n",
    "    # generator yields parameters for each module\n",
    "    def parameters(self):\n",
    "        for l in self._modules.values():\n",
    "            for p in l.parameters(): yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate DummyModule\n",
    "mdl = DummyModule(m,nh,10)\n",
    "# contents of mdl has the two linear layers\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([50, 784]),\n",
       " torch.Size([50]),\n",
       " torch.Size([10, 50]),\n",
       " torch.Size([10])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights and biases\n",
    "# shapes show that weights are stored as their transpose \n",
    "[o.shape for o in mdl.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the original `layers` approach, but we have to register the modules.\n",
    "Why? what does it mean to 'register' a module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original version of Model, the starting point for refactoring\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        # inherits from nn.Module, and super().__init__() allows access to all the methods and attributes of nn.Module\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out)]\n",
    "    # sequentially apply layers and return the output activations\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of layers to be implemented in Model\n",
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version of original Model, adds modules from an input list\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate Model\n",
    "#     but isn't the input supposed to be an object of class nn.Module()? \n",
    "#     layers is a list of such objects.\n",
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contents of model has all three layers\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.ModuleList` does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor using nn.ModuleList()\n",
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        # self.layers can act as an iterable\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate SequentialModel\n",
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1118, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute a training loop\n",
    "fit()\n",
    "# record the final loss and accuracy \n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` is a convenient class which does the same as the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch nn.Sequential()\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2136, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute a training loop, record the final loss and accuracy \n",
    "#     why are loss and accuracy worse than with SequentialModel()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Sequential??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace our previous manually coded optimization step:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```python\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params,self.lr = list(params),lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a model\n",
    "model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an optimizer\n",
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() function\n",
    "# training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i+bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            # compute predictions and loss function for this batch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD, and zero out the gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1280, grad_fn=<NllLossBackward>), tensor(0.9688))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides this exact functionality in `optim.SGD` (it also handles stuff like momentum, which we'll look at later - except we'll be doing it in a more flexible way!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export -- why do they keep saying export, when they are import ing?\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model and optimizer\n",
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3045, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()\n",
    "# compute loss function for a batch\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1502, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized tests can be very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[start_i:end_i]\n",
    "    yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a `Dataset` class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export -- again, what is meant by export here?\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() once more\n",
    "# training loop\n",
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs + 1):\n",
    "            xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "            # compute predictions and loss function for this batch\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            # backpropagation: compute gradients of the loss function\n",
    "            loss.backward()\n",
    "            # update the parameters using SGD, and zero out the gradients\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0330, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```python\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    ...\n",
    "```\n",
    "\n",
    "Let's make our loop much cleaner, using a data loader:\n",
    "\n",
    "```python\n",
    "for xb,yb in train_dl:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    # bs is batch size, and ds is the data set\n",
    "    def __init__(self, ds, bs): self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        # iterate through data set sequentially generating the next batch \n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put training and validation data in iterables that generate next batch\n",
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the first batch\n",
    "xb,yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs,28*28)\n",
    "assert yb.shape==(bs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the first image in the first batch\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model,opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor fit() using DataLoader class\n",
    "# training loop\n",
    "def fit():\n",
    "    # loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # loop over batches generated by DataLoader\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0660, grad_fn=<NllLossBackward>), tensor(1.))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model for 1 epoch\n",
    "fit()\n",
    "# compute loss and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our training set to be in a random order, and that order should differ each iteration. But the validation set shouldn't be randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator yields indices for random batches from data set\n",
    "class Sampler():\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n,self.bs,self.shuffle = len(ds),bs,shuffle\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # torch.randperm(n) returns a random permutation of integers 0:n-1\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset takes two inputs: *train_ds means both elements of train_ds, i.e. the training examples and the labels\n",
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4]))\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# first 10 examples and their labels\n",
    "print(train_ds[:10])\n",
    "print(len(train_ds[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches with no shuffle, bs = 3\n",
    "# returns indexes of examples\n",
    "# note the last batch has only 1 element\n",
    "s = Sampler(small_ds,3,False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([3, 0, 8]), tensor([6, 7, 1]), tensor([4, 9, 5]), tensor([2])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches, with shuffle\n",
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([6, 2, 4]), tensor([9, 5, 3]), tensor([7, 8, 1]), tensor([0])]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample an epoch of training data in batches again, with shuffle\n",
    "s = Sampler(small_ds,3,True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs,ys = zip(*b)\n",
    "    return torch.stack(xs),torch.stack(ys)\n",
    "\n",
    "# refactor DataLoader()\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a random sampler for the training set, and a non-random sampler for the validation set\n",
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data loaders\n",
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a batch from the validation data and show the first image\n",
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOLklEQVR4nO3dX4wd9XnG8efBEAuIkflnahvTpBFILUhgsBAIVKWKMBQUTC5SYkxNZagjiEuQIigyFwH5AmhLIyQQwhEmxsKOIggKFxGNhZBoxR+ztijYIGKKwNnY8pZigQMSqfHbix1Xi9n5zXLm/LPf70danXPmPTPz6uw+O3POnJmfI0IADn9HDLoBAP1B2IEkCDuQBGEHkiDsQBJH9nNltvnoH+ixiPBk01tt2W1fZvst22/bvr3NsgD0ljs9zm57mqTfSrpE0qikVyQtjog3CvOwZQd6rBdb9vMlvR0R70TEHyX9XNKiFssD0ENtwj5X0u8mPB6tpn2O7eW2R2yPtFgXgJbafEA32a7CF3bTI2K1pNUSu/HAILXZso9Kmjfh8amSdrZrB0CvtAn7K5JOt/1121+R9D1JT3enLQDd1vFufETss71C0r9JmiZpTURs61pnALqq40NvHa2M9+xAz/XkSzUADh2EHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ6Hp9dkmy/K2mvpM8k7YuIBd1oCkD3tQp75a8i4v0uLAdAD7EbDyTRNuwh6Te2N9tePtkTbC+3PWJ7pOW6ALTgiOh8ZntOROy0PUvSRkn/EBHPF57f+coATElEeLLprbbsEbGzuh2T9JSk89ssD0DvdBx228fannHgvqSFkrZ2qzEA3dXm0/hTJD1l+8By1kfEM13pCl/KkiVLamvXXnttcd6FCxcW60ccUd4e7N+/v1gvrX/Dhg3FedFdHYc9It6RdHYXewHQQxx6A5Ig7EAShB1IgrADSRB2IIlW36D70ivjG3STuuqqq4r12267rVifP39+be2oo47qqKcDqkOrtZr+fjZt2lRbu+KKK4rz7tmzp1jH5HryDToAhw7CDiRB2IEkCDuQBGEHkiDsQBKEHUiC4+xdcNpppxXry5YtK9ZvvfXWYn369OnF+ieffFJbe/zxx4vzrlq1qlhvsn79+mL94osvrq099thjxXmbXjdMjuPsQHKEHUiCsANJEHYgCcIOJEHYgSQIO5BENwZ2TG/dunXF+kUXXdRq+S+//HKxvnVr/eX6b7zxxlbrbrJt27ZivXSc/bjjjut2O58zY8aM2lrT7+SZZw6/q6KzZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDjOPkU333xzbe3cc89tteyPPvqoWG863/2FF15otf42SsNFN3nooYdarfvMM88s1kvn8p9xxhnFeZcuXVqsP/HEE8X6MGrcstteY3vM9tYJ006wvdH29ur2+N62CaCtqezG/0zSZQdNu13SsxFxuqRnq8cAhlhj2CPieUkfHDR5kaS11f21ksrjFwEYuE7fs58SEbskKSJ22Z5V90TbyyUt73A9ALqk5x/QRcRqSaulw/eCk8ChoNNDb7ttz5ak6nasey0B6IVOw/60pOuq+9dJ+lV32gHQK4278bY3SPqmpJNsj0r6saR7JP3C9vWSdkj6bi+b7IemMdLvvvvu2lrTdd23bNlSrN90003F+sjISLHeS4sXLy7Wjz766GJ948aNtbWXXnqpOO+VV15ZrK9cubJYP+uss2pr77//fnHept4ORY1hj4i63/a3utwLgB7i67JAEoQdSIKwA0kQdiAJwg4kwSmulblz5xbrTYfXSt56661ivZeH1qZNm1asr1ixoli/4YYbWi3/vffeq61ddtnB51d93sMPP1ysz5w5s1gveeSRR4r10dHRjpc9rNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGfvg/vvv7+nyz/++PqL+z7wwAPFea+++upi3XaxHlG++NB5551XW2s6ht/Wvn37amtNpx0fjtiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGfvg29/+9vFetvz2Y855pjaWtNx9F6bP3/+wNa9bdu22tqTTz7Zx06GA1t2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC4+x9sGTJkmL9yCPb/RpmzJjRav5eKp03Pm/evOK8J598crHedG33puGms2ncstteY3vM9tYJ0+60/Xvbr1Y/l/e2TQBtTWU3/meSJhu64ycRcU718+vutgWg2xrDHhHPS/qgD70A6KE2H9CtsP1atZtfexE028ttj9ju3YBmABp1GvaHJH1D0jmSdkm6r+6JEbE6IhZExIIO1wWgCzoKe0TsjojPImK/pJ9KOr+7bQHoto7Cbnv2hIffkbS17rkAhoObrvtte4Okb0o6SdJuST+uHp8jKSS9K+n7EbGrcWV2eWVDbN26dbW1a665po+d9Ne9995brL/44ovFeuma9o8++mhHPR2wadOmYv3CCy9stfxDVURMerH/xm9zRMRk30woj2QPYOjwdVkgCcIOJEHYgSQIO5AEYQeS4BTXKVq2bFltbfv27cV5zz777GJ90aJFxfqePXuK9fXr19fWPv300+K8q1atKtb37t1brDd58MEHa2tNh32b3HXXXa3mz4YtO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0XiKa1dXdgif4trG9OnTi/UTTzyxWN+3b1+xPjY29qV76pcdO3bU1ubMmdNq2Zdcckmx/txzz7Va/qGq7hRXtuxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kATns/dB0znlO3fu7FMn3dc0HPWsWbM6XnbTZaqbLiWNz2PLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcJwdrcycObNYP/LIzv/E7rvvvmL9448/7njZGTVu2W3Ps/2c7Tdtb7P9w2r6CbY32t5e3dYPxA1g4KayG79P0o8i4s8lXSDpB7b/QtLtkp6NiNMlPVs9BjCkGsMeEbsiYkt1f6+kNyXNlbRI0trqaWslXdWrJgG096XeUNn+mqT5kl6WdEpE7JLG/yHYnvRL0LaXS1rerk0AbU057La/KulJSbdExEf2pNe0+4KIWC1pdbWMlBecBIbBlA692T5K40F/PCJ+WU3ebXt2VZ8taXgvcQqg+VLSHt+Er5X0QUTcMmH6P0v6n4i4x/btkk6IiNsalsWW/TBz6qmnFuubN2+urTVdQvuCCy4o1kdGRor1rOouJT2V3fiLJP2tpNdtv1pNWynpHkm/sH29pB2SvtuNRgH0RmPYI+I/JNW9Qf9Wd9sB0Ct8XRZIgrADSRB2IAnCDiRB2IEkOMUVrYyOjhbrTZfRRv+wZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDjOjp768MMPa2tz5swpzts0HPSOHTuK9bExrqcyEVt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii8brxXV0Z141PZ+nSpbW1NWvWtFr2rl27ivVLL720tvbGG2+0Wvcwq7tuPFt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii8Xx22/MkPSbpTyTtl7Q6Iu63faekv5f039VTV0bEr3vVKA5NW7Zsqa3dcccdxXkXLlxYrK9du7ZYP5yPpXdiKhev2CfpRxGxxfYMSZttb6xqP4mIf+ldewC6ZSrjs++StKu6v9f2m5Lm9roxAN31pd6z2/6apPmSXq4mrbD9mu01to+vmWe57RHbI606BdDKlMNu+6uSnpR0S0R8JOkhSd+QdI7Gt/z3TTZfRKyOiAURsaAL/QLo0JTCbvsojQf98Yj4pSRFxO6I+Cwi9kv6qaTze9cmgLYaw27bkh6R9GZE/OuE6bMnPO07krZ2vz0A3dJ4iqvtiyX9u6TXNX7oTZJWSlqs8V34kPSupO9XH+aVlsUprkCP1Z3iyvnswGGG89mB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJTOXqst30vqT3Jjw+qZo2jIa1t2HtS6K3TnWztz+tK/T1fPYvrNweGdZr0w1rb8Pal0RvnepXb+zGA0kQdiCJQYd99YDXXzKsvQ1rXxK9daovvQ30PTuA/hn0lh1AnxB2IImBhN32Zbbfsv227dsH0UMd2+/aft32q4Men64aQ2/M9tYJ006wvdH29up20jH2BtTbnbZ/X712r9q+fEC9zbP9nO03bW+z/cNq+kBfu0JffXnd+v6e3fY0Sb+VdImkUUmvSFocEUMxmLbtdyUtiIiBfwHD9l9K+oOkxyLirGraP0n6ICLuqf5RHh8R/zgkvd0p6Q+DHsa7Gq1o9sRhxiVdJenvNMDXrtDX36gPr9sgtuznS3o7It6JiD9K+rmkRQPoY+hFxPOSPjho8iJJa6v7azX+x9J3Nb0NhYjYFRFbqvt7JR0YZnygr12hr74YRNjnSvrdhMejGq7x3kPSb2xvtr180M1M4pQDw2xVt7MG3M/BGofx7qeDhhkfmteuk+HP2xpE2CcbmmaYjv9dFBHnSvprST+odlcxNVMaxrtfJhlmfCh0Ovx5W4MI+6ikeRMenypp5wD6mFRE7KxuxyQ9peEbinr3gRF0q9uxAffz/4ZpGO/JhhnXELx2gxz+fBBhf0XS6ba/bvsrkr4n6ekB9PEFto+tPjiR7WMlLdTwDUX9tKTrqvvXSfrVAHv5nGEZxrtumHEN+LUb+PDnEdH3H0mXa/wT+f+SdMcgeqjp688k/Wf1s23QvUnaoPHduv/V+B7R9ZJOlPSspO3V7QlD1Ns6jQ/t/ZrGgzV7QL1drPG3hq9JerX6uXzQr12hr768bnxdFkiCb9ABSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBL/B8XzXjbtoXgqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw a random batch from the training data\n",
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw another random batch from the training data\n",
    "xb,yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4210, grad_fn=<NllLossBackward>), tensor(0.8906))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the model and train it for a single epoch\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "# loss function and accuracy\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch to build data loaders\n",
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random batch\n",
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1946, grad_fn=<NllLossBackward>), tensor(0.9219))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model,opt = get_model()\n",
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's defaults work fine for most things however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, shuffle=True, drop_last=True)\n",
    "valid_dl = DataLoader(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random batch\n",
    "xb,yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2214, grad_fn=<NllLossBackward>), tensor(0.9531))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb), accuracy(model(xb), yb)\n",
    "assert acc>0.7\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch's `DataLoader`, if you pass `num_workers`, will use multiple threads to call your `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You **always** should also have a [validation set](http://www.fast.ai/2017/11/13/validation-sets/), in order to identify if you are overfitting.\n",
    "\n",
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call `model.train()` before training, and `model.eval()` before inference, because these are used by layers such as `nn.BatchNorm2d` and `nn.Dropout` to ensure appropriate behaviour for these different phases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # training phase: handle batchnorm / dropout\n",
    "        model.train()\n",
    "        # print(model.training)\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        # inference phase\n",
    "        model.eval()\n",
    "        # print(model.training)\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc = 0.,0.\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb)\n",
    "                tot_acc  += accuracy (pred,yb)\n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv, tot_acc/nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: Are these validation results correct if batch size varies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dls` returns dataloaders for the training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# why double batch size for validation data set? \n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.1746) tensor(0.9502)\n",
      "1 tensor(0.1228) tensor(0.9636)\n",
      "2 tensor(0.1310) tensor(0.9583)\n",
      "3 tensor(0.1062) tensor(0.9688)\n",
      "4 tensor(0.1116) tensor(0.9695)\n"
     ]
    }
   ],
   "source": [
    "# make the data loaders\n",
    "train_dl,valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "# instantiate model and optimizer\n",
    "model,opt = get_model()\n",
    "# run the model for 5 epochs, compute loss and accuracy\n",
    "loss,acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc>0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_minibatch_training.ipynb to exp\\nb_03.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 03_minibatch_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
