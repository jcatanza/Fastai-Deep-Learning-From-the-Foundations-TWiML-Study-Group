
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/10b_mixup_label_smoothing_jcat.ipynb

from exp.nb_10 import *

from torch.distributions.beta import Beta

def unsqueeze(input, dims):
    for dim in listify(dims): input = torch.unsqueeze(input, dim)
    return input

def reduce_loss(loss, reduction='mean'):
    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss

class NoneReduce():
    def __init__(self, loss_func):
        self.loss_func,self.old_red = loss_func,None

    def __enter__(self):
        if hasattr(self.loss_func, 'reduction'):
            self.old_red = getattr(self.loss_func, 'reduction')
            setattr(self.loss_func, 'reduction', 'none')
            return self.loss_func
        else: return partial(self.loss_func, reduction='none')

    def __exit__(self, type, value, traceback):
        if self.old_red is not None: setattr(self.loss_func, 'reduction', self.old_red)

class MixUp(Callback):

    _order = 90 #Runs after normalization and cuda
    def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α]))

    # save the current self.run.loss_func function, then replace it with self.loss_func
    def begin_fit(self):
        self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func

    # implement mixup step, if in prediction mode
    def begin_batch(self):
        # apply mixup only during the training phase
        if not self.in_train:
            return
        # draw a vector of samples from the beta distribution with α = β = 0.4
        λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device)
        λ = torch.stack([λ, 1-λ], 1)
        self.λ = unsqueeze(λ.max(1)[0], (1,2,3))
        # shuffle the batch and its labels on the GPU
        shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device)
        xb_shuffled,self.yb_shuffled = self.xb[shuffle],self.yb[shuffle]
        # form an affine linear combination of the batch and the shuffled batch
        #      using coefficients drawn from the beta distribution with α = β = 0.4
        self.run.xb = lin_comb(self.xb, xb_shuffled, self.λ)

    # restore the old loss function
    def after_fit(self): self.run.loss_func = self.old_loss_func

    # form the mixup loss function
    def loss_func(self, pred, yb):
        if not self.in_train:
            return self.old_loss_func(pred, yb)
        with NoneReduce(self.old_loss_func) as loss_func:
            loss_original_batch = loss_func(pred, yb)
            loss_shuffled_batch = loss_func(pred, self.yb_shuffled)
        # MixUp loss function
        loss = lin_comb(loss_original_batch, loss_shuffled_batch, self.λ)
        # change the loss function's 'reduction' attibute back to 'mean'
        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))

class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, ε:float=0.1, reduction='mean'):
        super().__init__()
        self.ε,self.reduction = ε,reduction

    def forward(self, output, target):
        # inputs:
        #      output is the activations from the last layer of the model, size is [64, 10]
        #      target is the 'true' labels, size is [64]

        # c is the number of classes for the imagenette data set, which is 10
        c = output.size()[-1]

        # pytorch's fast and numerically stable computation of log_softmax
        #     Note: -log_preds has size [64,10]
        log_preds = F.log_softmax(output, dim=-1)

        # one-hot encode the target vector
        mask = torch.nn.functional.one_hot(target.long(), num_classes=c)

        # zero-hot encode the target vector, i.e. a mask of the "incorrect" classes
        mask = (mask==0).float()

        # sum of the cross-entropy loss over the "incorrect" classes, assuming each is the true class
        # sum_class_loss = -log_preds.sum(dim=-1) # (original implementation)
        sum_class_loss = (-log_preds*mask).sum(dim=-1) # corrected

        # mean of sum_class_loss for the batch
        loss = reduce_loss(sum_class_loss, self.reduction)

        # negative log-likelihood loss, averaged over the batch
        nll = F.nll_loss(log_preds, target, reduction=self.reduction)

        # return lin_comb(loss/c, nll, self.ε) # original implementation
        return lin_comb(loss/(c-1), nll, self.ε) # corrected implementation