
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/04_callbacks_jcat.ipynb

from exp.nb_03 import *

class DataBunch():
    def __init__(self, train_dl, valid_dl, n_out=None):
        self.train_dl,self.valid_dl,self.n_out = train_dl,valid_dl,n_out

    # add train_ds() as an attribute
    @property
    def train_ds(self): return self.train_dl.dataset


    # add valid_ds() as an attribute
    @property
    def valid_ds(self): return self.valid_dl.dataset


# instantiates the model and the optimizer, given the data and parameters
def get_model(data, learning_rate=0.5, n_hidden = 50):
    n_columns = data.train_ds.x.shape[1]
    n_out = data.n_out
    model = nn.Sequential(nn.Linear(n_columns,n_hidden), nn.ReLU(), nn.Linear(n_hidden,n_out))
    # Q: why can we access the optimizer from within this function?
    return model, optim.SGD(model.parameters(), lr=learning_rate)

# the Learner() class is a container for the model, optimization, loss function and data
class Learner():
    def __init__(self, model, opt, loss_func, data):
        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data

import re

# helper function uses regular expressions to transform a CamelCase callback name to snake_case
_camel_re1 = re.compile('(.)([A-Z][a-z]+)')
_camel_re2 = re.compile('([a-z0-9])([A-Z])')
def camel2snake(name):
    s1 = re.sub(_camel_re1, r'\1_\2', name)
    return re.sub(_camel_re2, r'\1_\2', s1).lower()

# refactored Callback() class
class Callback():

    # initialize _order to zero.
    _order=0

    # set_runner() method takes a callback as an input
    #     note that initially self.run is unset -- there is no default value
    def set_runner(self, run):
        self.run=run

    def __getattr__(self, callback_name):
        return getattr(self.run, callback_name)

    # set the callback name property
    #     if the callback doesn't have a name, set the callback name property to 'callback'
    @property
    def name(self):
        name = re.sub(r'Callback$', '', self.__class__.__name__)
        return camel2snake(name or 'callback')
        # note: the above line is equivalent to the following block
        '''
        try:
            return camel2snake(name)
        except:
            return 'callback'
        '''


class TrainEvalCallback(Callback):

    # initialize the epoch, batch, and iteration counters
    def begin_fit(self):
        # n_epoch_float keeps track of fractional number of elapsed epochs
        self.run.n_epoch_float = 0.
        self.run.n_batch = 0
        self.run.n_iter = 0

    # if we are in the training phase, update the epoch and batch counters
    def after_batch(self):
        if not self.in_train:
            return
        # each batch represents a fraction of an epoch
        self.run.n_epoch_float += 1./self.n_batches
        self.run.n_batch   += 1

    # execute the training phase
    def begin_epoch(self):
        self.run.n_epoch_float=self.n_epoch_float
        self.model.train()
        self.run.in_train=True

    # execute the prediction phase
    def begin_validate(self):
        self.model.eval()
        self.run.in_train=False

from typing import *

# helper function to convert any input into a list
def listify(o):
    if o is None: return []
    if isinstance(o, list): return o
    if isinstance(o, str): return [o]
    if isinstance(o, Iterable): return list(o)
    return [o]


class Runner():
    # initialize by setting the stop Flag to False, and constructing a list of callbacks from the inputs
    def __init__(self, callbacks=None, callback_funcs=None):
        # inputs are two lists: callbacks and callback_funcs
        # Q: it's not clear why we need two lists rather than one
        # create a list of callbacks from the input callbacks
        callbacks = listify(callbacks)
        # associate each callback_func() to its snake case callback name, then append it to the callbacks list
        for callback_func in listify(callback_funcs):
            callback = callback_func()
            setattr(self, callback.name, callback)
            callbacks.append(callback)
        # set the stopping flag to `False` and append TrainEvalCallback() to the callbacks list
        self.stop,self.callbacks = False,[TrainEvalCallback()]+callbacks

    # get the properties of the Learner object
    @property
    def opt(self):       return self.learn.opt
    @property
    def model(self):     return self.learn.model
    @property
    def loss_func(self): return self.learn.loss_func
    @property
    def data(self):      return self.learn.data

    # method to process a single batch
    def one_batch(self, xb, yb):
        self.xb,self.yb = xb,yb
        if self('begin_batch'):
            return
        # run the model
        self.pred = self.model(self.xb)
        if self('after_pred'):
            return
        # compute the loss function
        self.loss = self.loss_func(self.pred, self.yb)
        if self('after_loss') or not self.in_train:
            return
        # do backpropagation
        self.loss.backward()
        if self('after_backward'):
            return
        # update parameters
        self.opt.step()
        if self('after_step'):
            return
        # zero the gradients to prepare for the next batch
        self.opt.zero_grad()

    # method to process all batches
    def all_batches(self, dataloader):
        # total number of batches in an epoch
        self.n_batches = len(dataloader)
        # self.n_epoch_float = 0.
        for xb,yb in dataloader:
            # break if run.stop flag has been set
            if self.stop:
                break
            # process the next batch, then run the `after_batch` callback
            self.one_batch(xb, yb)
            self('after_batch')
        # set the run.stop flag to `False`
        self.stop=False

    # method to process training or validation data
    def fit(self, learn, n_epochs):
        self.n_epochs,self.learn = n_epochs,learn

        try:
            # loop over all callbacks in list and set_runner for each one
            for callback in self.callbacks:
                callback.set_runner(self)
            if self('begin_fit'):
                return
            for epoch_number in range(n_epochs):
                self.epoch_number = epoch_number

                # training phase
                if not self('begin_epoch'):
                    self.all_batches(self.data.train_dl)

                # validation phase
                with torch.no_grad():
                    if not self('begin_validate'):
                        self.all_batches(self.data.valid_dl)
                # break if `after_epoch` state is `True`
                if self('after_epoch'):
                    break

        finally:
            # set the `after_fit` state to `True`
            self('after_fit')
            # erase the Learner object
            self.learn = None

    def __call__(self, callback_name):
        # __call__ allows an instance of this class to be called as a function
        # loop through the callback list, return True if the requested callback callback_name is present,
        #     otherwise return False
        for callback in sorted(self.callbacks, key=lambda x: x._order):
            # check this callback name, and return True if it is the requested callback
            # get the callback associated with callback_name, otherwise return None
            f = getattr(callback, callback_name, None)
            if f and f(): # guarantees that the callback is present and is a function
                return True
        return False

class AvgStats():
    def __init__(self, metrics, in_train):
        self.metrics,self.in_train = listify(metrics),in_train

    # initialize total_loss and count to zero, and total_metrics to zeros for each metric
    def reset(self):
        # count keeps track of total samples processed
        self.total_loss,self.count = 0.,0
        self.total_metrics = [0.] * len(self.metrics)

    # combine loss and metrics
    @property
    def all_stats(self):
        # all_stats is a list containing loss and all metrics
        # Q: why does total_loss have to be extracted with .item()
        return [self.total_loss.item()] + self.total_metrics

    # compute avg loss and metrics per sample
    @property
    def avg_stats(self):
        # each stat is averaged over the number of samples
        return [o/self.count for o in self.all_stats]

    # compute and display stats
    def __repr__(self):
        if not self.count:
            return ""
        return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"

    def accumulate(self, run):
        # get the number of samples in this batch
        n_samples_in_batch = run.xb.shape[0]
        # weight the loss function for the batch by the number of samples in the batch
        self.total_loss += run.loss * n_samples_in_batch
        # accumulate count of samples processed
        self.count += n_samples_in_batch
        # accumulate the metrics, weighting each by number of samples in the batch
        for i,metric in enumerate(self.metrics):
            self.total_metrics[i] += metric(run.pred, run.yb) * n_samples_in_batch

class AvgStatsCallback(Callback):
    def __init__(self, metrics):
        self.train_stats,self.valid_stats = AvgStats(metrics,in_train=True),AvgStats(metrics,in_train=False)

    # initialize train_stats and valid_stats at the start of an epoch
    def begin_epoch(self):
        self.train_stats.reset()
        self.valid_stats.reset()

    # compute and accumulate stats after the loss function has been evaluated
    def after_loss(self):
        stats = self.train_stats if self.in_train else self.valid_stats
        with torch.no_grad():
            stats.accumulate(self.run)
    # print stats after the epoch has been processed
    def after_epoch(self):
        print(self.train_stats)
        print(self.valid_stats)

from functools import partial